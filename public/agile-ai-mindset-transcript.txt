Julio Zelaya: So, good evening, everybody, and welcome to our session of today. I'm looking forward to this session, the Agile AI Mindset, with the great Josh Pincell. You will see he is extremely engaging, and he's a great friend. So, before I present Josh.9600:14:05.380 --> 00:14:13.019Julio Zelaya: Let's do a brief recap of last week's session. So, last week's session with Dr. Chen.9700:14:13.020 --> 00:14:20.349Julio Zelaya: we had one main conclusion, that we need to shift from AI, artificial intelligence, to9800:14:20.410 --> 00:14:44.439Julio Zelaya: IA, Intelligence Augmentation. So, educators in the center as designers, and also our learners in the center. Not just users of technology, but actually co-creating. We had great conversations regarding the evolution of the AI landscape. We also saw some of the tools that we're using9900:14:44.440 --> 00:14:49.730Julio Zelaya: and some of the examples that we saw with Dr. Chen, and also our own examples.10000:14:49.730 --> 00:15:06.400Julio Zelaya: and some frameworks that we learned. So we had a very exciting session. We're more knowledgeable about where the ecosystem is at. So today will be a very applied session as well. So before we start, let's get into some current news.10100:15:06.400 --> 00:15:12.790Julio Zelaya: I always love to start with news to see that the AI landscape is changing every single day.10200:15:12.790 --> 00:15:23.640Julio Zelaya: Last week, we talked about OpenAI infringing copyright. We then saw that Anthropic has just reached a settlement with writers.10300:15:23.640 --> 00:15:34.070Julio Zelaya: a $1.5 billion settlement, one of the largest ever. Now it's Apple being sued by neuroscientists that Apple was using10400:15:34.070 --> 00:15:43.629Julio Zelaya: pirated books for their training data. So, as we're seeing, all of the big tech companies are having the same struggles regarding data.10500:15:43.770 --> 00:15:54.330Julio Zelaya: And talking about data, Anthropic just released a study that only 250 documents, if you can poison an AI model.10600:15:54.330 --> 00:16:04.700Julio Zelaya: So, just by uploading and retraining, just with 250 corrupted documents, you can have any large language model to produce gibberish.10700:16:04.720 --> 00:16:23.800Julio Zelaya: And we've seen that in this past week, there's… there have been a lot of cyber attacks in companies, so this is not the exception for the large language models. So you can imagine that a lot of the users are saying, okay, if I cannot trust this, how can I use it in different applications?10800:16:23.980 --> 00:16:30.719Julio Zelaya: But in other news, Musk is building a new type of AI to predict10900:16:30.980 --> 00:16:53.240Julio Zelaya: actual worlds, and they're hiring people to build games, but there's an ultimate… there's an ultimate strategy by building a game, and actual worlds. Can you figure out what's the real strategy behind building actual worlds? Because it's a very clever strategy. Why does he want to…11000:16:53.240 --> 00:16:56.780Julio Zelaya: to build worlds, for AI to to…11100:16:57.170 --> 00:17:04.460Julio Zelaya: to train upon, and also creating games. Does anybody figure out what the ultimate strategy is?11200:17:06.119 --> 00:17:11.999Andrew Morgan: To predict human behavior and kind of game out scenarios before they actually happen.11300:17:12.180 --> 00:17:14.209Julio Zelaya: That's part of the equation.11400:17:14.579 --> 00:17:17.579Julio Zelaya: What's the other one? Because that's part of it.11500:17:17.839 --> 00:17:19.099Julio Zelaya: for humans.11600:17:19.349 --> 00:17:20.639Julio Zelaya: What's the other one?11700:17:24.750 --> 00:17:29.600Julio Zelaya: Da-da-da! So, the ultimate game is to train11800:17:29.630 --> 00:17:44.530Julio Zelaya: robots, and they're trying to build worlds for robot training. And there's this company that has just launched this robot, available 2026, and pre-orders11900:17:44.530 --> 00:18:01.569Julio Zelaya: around $25,000. If you don't like to fold your clothes, maybe $25,000 can fix that problem in 2026. So this is part. And also, Tesla is saying that12000:18:01.780 --> 00:18:12.480Julio Zelaya: due to a lot of cost plunges, they can have a lot of robots available in the future, especially for factories. And this is already12100:18:12.610 --> 00:18:13.680Julio Zelaya: happening.12200:18:13.790 --> 00:18:27.900Julio Zelaya: Yes, the demos. You'll see the robots there. Sometimes it struggles with folding delicates, but this is my ultimate dream come true, that if I can have help around the house with this.12300:18:28.340 --> 00:18:43.040Julio Zelaya: this would be a great game changer, at least for me. So, my friends, this is just to say that we need to have an agile mindset, and everything is changing very rapidly, and for12400:18:43.040 --> 00:18:56.949Julio Zelaya: Teaching us and guiding us in this… in this journey. I have the great pleasure of introducing a great friend, and I've seen him in action. He's amazing, one of the best, my dear Josh Penzel.12500:18:56.950 --> 00:19:06.729Audio shared by Julio Zelaya: How do you turn AI from a buzzword into $2 million in real business impact? Josh Penzel knows exactly how, and he's about to show you.12600:19:06.990 --> 00:19:18.719Audio shared by Julio Zelaya: From launching Alexa to 10 countries, to revolutionizing corporate training with AI, Josh has spent 15 years at the intersection of technology, learning, and business strategy.12700:19:18.720 --> 00:19:43.300Audio shared by Julio Zelaya: As Vice President of AI Strategy at ELB Learning, he's the bridge between cutting-edge AI and real-world results. $500,000 in direct AI revenue, $1.5 million in influence business, 50-plus keynotes on generative AI in just 12 months, Fortune 500 clients transformed. His secret? Making AI practical, profitable, and accessible.12800:19:43.540 --> 00:20:06.540Audio shared by Julio Zelaya: From reducing onboarding time by 65% at Zoox, to increasing customer satisfaction by 25% at Skillsoft. Josh doesn't just talk AI, he delivers results. Industry thought leader, executive advisor, change catalyst. When Amazon needed to train 10,000 plus people on AI devices, they turned to Josh.12900:20:06.960 --> 00:20:11.800Audio shared by Julio Zelaya: When companies need to transform their workforce with AI, he's their guide.13000:20:12.110 --> 00:20:30.229Audio shared by Julio Zelaya: He's trained everyone, from engineers to executives, to embrace the AI revolution. In The Agile AI Mindset, Josh will transform how you think about AI, move fast without breaking things, turn AI experiments into sustainable solutions.13100:20:30.240 --> 00:20:48.090Audio shared by Julio Zelaya: Build a culture of AI innovation in your school. Measure what matters. Real impact on students. Learn the mindset that turns AI skeptics into AI champions. Get ready to develop the agile mindset that's transformed organizations worldwide.13200:20:48.520 --> 00:20:52.250Audio shared by Julio Zelaya: Your AI journey accelerates with Josh Penzel.13300:20:54.930 --> 00:20:57.069Audio shared by Julio Zelaya: Welcome, my dear friend! Wow!13400:20:57.080 --> 00:21:00.050Julio Zelaya: You have a great group of leaders…13500:21:00.050 --> 00:21:01.000Josh Penzell: Gosh.13600:21:01.000 --> 00:21:02.779Julio Zelaya: Welcome, my dear friend.13700:21:03.230 --> 00:21:10.770Josh Penzell: They usually say you're supposed to under-deliver, you know, under-promise and over-deliver. You may have Julio,13800:21:11.070 --> 00:21:22.129Josh Penzell: put me in a pickle here. Wow, what an incredible… well, hey, thanks, everyone. How's everyone doing? Can I just get some, you know, some love in the world? Yeah, you might have seen I have a theater13900:21:22.130 --> 00:21:33.939Josh Penzell: you know, one thing I don't think was mentioned there, which I think is interesting, I have a Master of Fine Arts in theater, and I was a musical theater director, before I went on and launched Alexa to the entire globe,14000:21:33.950 --> 00:21:37.680Josh Penzell: a prism of a life school, but that means, like.14100:21:37.770 --> 00:21:43.979Josh Penzell: you know, I'm a bit, I like the energy, right? So, feel free to…14200:21:44.100 --> 00:21:50.999Josh Penzell: you know, come off, enjoy, do… at least when I was in theater school, or when I was getting my graduate degree, we were like, you know.14300:21:51.110 --> 00:21:55.630Josh Penzell: acting like monkeys and stuff. I won't ask y'all to do that, but,14400:21:55.760 --> 00:22:08.020Josh Penzell: we might try to, bring some humanity here to… to AI. So, wow, Julio, thank you. Okay, so is it off… is it… is it on me now? Oh, that's me seeing myself.14500:22:09.370 --> 00:22:17.949Josh Penzell: Right. Well, hey, let me, I'll go ahead and share my… my screen. But hey, while I'm doing that, actually, before I do that, can I just get everyone in the,14600:22:18.160 --> 00:22:30.149Josh Penzell: In the chat, or whatever, if your mood right now, after that incredible video, was a, any hairstyle from history, what hairstyle would it be?14700:22:31.880 --> 00:22:36.090Josh Penzell: So if your mood right now was any hairstyle from history, what…14800:22:36.620 --> 00:22:40.589Josh Penzell: What hairstyle would it be? A mullet! I love that.14900:22:41.800 --> 00:22:43.430Josh Penzell: What else? What else do we have?15000:22:46.520 --> 00:22:50.229Josh Penzell: Well, why a mullet, can I ask, while other people figure out why?15100:22:51.300 --> 00:22:53.580rthurman: I've… I'm mullet, ready to go.15200:22:53.730 --> 00:22:54.709Josh Penzell: You're ready to go.15300:22:54.710 --> 00:22:56.880rthurman: Business in the back, or whatever it was.15400:22:57.060 --> 00:23:00.819Josh Penzell: I love that. Alright, and we got pigtails. Why… why pigtails?15500:23:01.420 --> 00:23:02.110Josh Penzell: Oof.15600:23:02.500 --> 00:23:04.239Babette Faehmel: Pippi Longstocking.15700:23:04.650 --> 00:23:08.539Josh Penzell: Yeah, she's coming into your world. Am I right or am I right? Right?15800:23:08.820 --> 00:23:09.749Josh Penzell: And we… why do we got.15900:23:09.750 --> 00:23:11.850Babette Faehmel: I'm waiting. I'm waiting.16000:23:12.020 --> 00:23:20.190Josh Penzell: Fangs and a French bob, I love this. Alright, well, I'm gonna stop sharing so I can see all y'all's faces. What else?16100:23:26.680 --> 00:23:32.339Josh Penzell: I think for me, it's gonna be, like, Doc… usually what I would say is I'm, like, Doc Brown.16200:23:32.350 --> 00:23:49.459Josh Penzell: You know? And not only because, like, you know, I go a little nuts and crazy and all this, but also because by the end of the session, my hair will probably be… the moisture, right, will begin to throw this out, so I call that my conference here. But thanks. One of the reasons I'm actually asking this16300:23:49.710 --> 00:23:58.699Josh Penzell: first of all, one of my favorite openers, or icebreakers, right? But one of the reasons I'm going here is… one thing we're gonna talk about today16400:23:58.880 --> 00:24:02.090Josh Penzell: Is how subjective languages16500:24:03.840 --> 00:24:08.409Josh Penzell: And what we are doing in starting this is instead of saying, hey, how are you doing?16600:24:08.870 --> 00:24:19.449Josh Penzell: I'm actually trying to engage in a metaphor, right? I'm trying to get us to understand when you say mullet, what does that mean to you? Right? Because I've heard people say, for instance, oh, I'm a French braid.16700:24:19.600 --> 00:24:21.370Josh Penzell: Cause I'm put together.16800:24:21.670 --> 00:24:26.380Josh Penzell: And then I hear other people say, oh, I'm a French braid because I'm trying to keep it all together.16900:24:26.970 --> 00:24:32.930Josh Penzell: Right? And so what's so fascinating, and one thing we're going to explore today, and one of the magical things about generative AI,17000:24:33.260 --> 00:24:44.320Josh Penzell: is how… oh, that's gonna be a really interesting thing if it's zooming in and out like that all the time. I might even try blurring the background so you guys don't have to stare at my wild collection and my background. But…17100:24:44.520 --> 00:24:56.459Josh Penzell: you know, just like my background, we're all unique, right? We all have these sub… these things that make us specific, and therefore, when we communicate, we're communicating with all of that, I don't want to say baggage, but all of that stuff around us.17200:24:56.670 --> 00:25:00.520Josh Penzell: And so how does that incorporate with AI?17300:25:00.740 --> 00:25:04.410Josh Penzell: That's what we're gonna talk about today, and how does it actually mean17400:25:04.560 --> 00:25:13.339Josh Penzell: Might I add that understanding how humans behave and humans learn makes you more equipped To lead with this.17500:25:14.200 --> 00:25:17.269Josh Penzell: So, before I get into the main…17600:25:17.440 --> 00:25:22.489Josh Penzell: Let me just start with a question. Does a self-driving vehicle need windshield wipers?17700:25:23.100 --> 00:25:24.320Josh Penzell: What do you think?17800:25:26.320 --> 00:25:31.460Josh Penzell: Does a self-driving car or a self-driving vehicle need windshield wipers?17900:25:33.980 --> 00:25:43.619Josh Penzell: Does anyone want to… can I just get maybe in the chat, you can just say yes, no, maybe, yes, to keep the humans comfortable and allow them to see, yes for me?18000:25:43.870 --> 00:25:47.860Josh Penzell: Because the passengers like to view, it depends where the cameras are, okay?18100:25:48.340 --> 00:25:49.820Josh Penzell: Yeah, so…18200:25:49.920 --> 00:25:56.269Josh Penzell: The answer is, yes, the humans, yes, if I need to interfere. Fascinating. So here's the reason I ask.18300:25:56.420 --> 00:26:05.209Josh Penzell: is, I worked at an organization called Zoox, alright? I ran training there for a bit. These are running today, autonomously.18400:26:06.110 --> 00:26:11.190Josh Penzell: In… Las Vegas. So you can go to Las Vegas and actually hail one of these vehicles.18500:26:12.450 --> 00:26:15.279Josh Penzell: The reason I wanted to bring this up, though, is…18600:26:15.750 --> 00:26:22.030Josh Penzell: They said, and this was back in 2017, they came out of stealth around 2019,18700:26:22.190 --> 00:26:24.609Josh Penzell: They said if… if we don't…18800:26:25.040 --> 00:26:32.510Josh Penzell: If we're putting autonomous driving technology into a vehicle, why are we building vehicles for drivers?18900:26:34.190 --> 00:26:36.529Josh Penzell: And so they recreated the vehicle.19000:26:36.730 --> 00:26:39.819Josh Penzell: Right? Where the people could… Point to each other.19100:26:39.920 --> 00:26:46.230Josh Penzell: And then they said, well, if we're gonna rebuild our own vehicle, why don't we build it not to be owned19200:26:46.490 --> 00:26:50.300Josh Penzell: but rather… So that we could…19300:26:50.580 --> 00:26:55.849Josh Penzell: I don't know, travel it anywhere we want. And hey, if we're doing that, why do we even need a front and a back?19400:26:56.250 --> 00:26:58.890Josh Penzell: We'll just let the lights… Change.19500:27:00.690 --> 00:27:08.639Josh Penzell: And what they did, and this is what I try to, you know, think about is, they started at first principles. What's the goal? Is to get people from point A to point B.19600:27:09.750 --> 00:27:21.510Josh Penzell: Now, everyone talks about reducing, for instance, drivers on the road and all this, but what they've figured out is, you know what the biggest contributor to pollution in the world is when it comes to vehicles? Building them.19700:27:22.850 --> 00:27:25.629Josh Penzell: So, if I can get less people to own a vehicle.19800:27:26.890 --> 00:27:32.330Josh Penzell: I can do that, and I can do that because now I don't have to build a vehicle for someone who drives.19900:27:33.080 --> 00:27:36.349Josh Penzell: And what I love about this is it not just reimagines20000:27:37.720 --> 00:27:43.249Josh Penzell: Right? It reimagines everything, right? Not just taking the engine and putting it in the horse and buggy.20100:27:43.380 --> 00:27:48.230Josh Penzell: But new problems emerged. For instance, who does a pedestrian look at?20200:27:48.340 --> 00:27:50.010Josh Penzell: When they need to cross the road.20300:27:51.180 --> 00:27:55.390Josh Penzell: And by the way, the way they started experimenting with that was directive sound.20400:27:56.110 --> 00:27:59.510Josh Penzell: So, a sound would go… Or titty bid-boo!20500:27:59.680 --> 00:28:02.420Josh Penzell: And which one meant you may go versus warning?20600:28:03.600 --> 00:28:08.850Josh Penzell: They also were able to rebuild the vehicle, for instance, to Crabwalk, because it does not have a front or back.20700:28:10.800 --> 00:28:18.390Josh Penzell: So one of the things I want to challenge us to do, and that's part of what I think about in my work, right?20800:28:19.000 --> 00:28:24.260Josh Penzell: And, I think the artistry and my artistic background dive into this.20900:28:24.610 --> 00:28:42.099Josh Penzell: But how can we reimagine the value we're trying to bring to our learners, to our students? How can we, right, reimagine the work we do, not just take AI and put it into the existing work we do? Not just to have it make us do things quicker.21000:28:42.300 --> 00:28:45.919Josh Penzell: And so one thing you'll notice today is I'm not using a PowerPoint.21100:28:47.150 --> 00:28:49.220Josh Penzell: I've actually created a website.21200:28:50.120 --> 00:28:55.000Josh Penzell: The way I created that website was vibe coding, using AI,21300:28:56.080 --> 00:29:03.039Josh Penzell: And I made this… this is a version of one I made, right? But I can make this entire thing in less than an hour.21400:29:04.050 --> 00:29:09.840Josh Penzell: Not only that, but after this session, I'm going to send you the link. You're gonna be able to interact with it.21500:29:10.780 --> 00:29:18.219Josh Penzell: And right down here, there's a little AI assistant. Julio's gonna give me the transcript. I'm gonna put that transcript into this.21600:29:18.480 --> 00:29:22.269Josh Penzell: And you're gonna be able to talk with this session afterwards.21700:29:26.070 --> 00:29:28.579Josh Penzell: And I have a Master of Fine Arts in theater.21800:29:28.710 --> 00:29:31.300Josh Penzell: I don't know a lick of code, okay?21900:29:31.910 --> 00:29:34.729Josh Penzell: Although I've learned it through, you know, a little bit.22000:29:35.130 --> 00:29:41.020Josh Penzell: But I say that to say what I'm trying to do is instead of saying, hey, I'm gonna make a better PowerPoint.22100:29:41.820 --> 00:29:51.329Josh Penzell: What I thought is, what is the best way… what am I trying to do with the PowerPoint? What am I trying to do with that? I'm trying to present information to you. No. I want you to be able to walk out and do something with that information.22200:29:52.120 --> 00:30:03.330Josh Penzell: Right? So after this, this whole thing will be usable to you, and in some cases, I've made it as such that you can take it and use it afterwards, rather than it just being something I'm presenting with.22300:30:04.960 --> 00:30:05.820Josh Penzell: Alright?22400:30:06.920 --> 00:30:08.970Josh Penzell: With that, let's build a story together.22500:30:11.380 --> 00:30:16.860Josh Penzell: What I mean is, let's play a little game that I think is gonna tell us a little more about how AI22600:30:17.250 --> 00:30:18.160Josh Penzell: works.22700:30:18.410 --> 00:30:27.609Josh Penzell: Okay? But you'll have to go down on a little theatrical journey with me for a second. So who… can I get maybe 2 or 3… let's get 3 volunteers.22800:30:27.900 --> 00:30:30.930Josh Penzell: Okay, and you don't have to come off mute if you don't want to.22900:30:31.200 --> 00:30:41.959Josh Penzell: But how about we do it in chat? I just need 3 volunteers, so 3 people, raise your hands, and what we're gonna do is, it's gonna be very simple, I promise. Just need 3, your digital hands.23000:30:42.680 --> 00:30:49.020Josh Penzell: Good, I got… thank you. Is it John… can you please pronounce the name so I don't mispronounce it cruelly?23100:30:49.020 --> 00:30:51.250Debbie Li: No problem, Josh. My name's Debbie.23200:30:51.550 --> 00:30:56.010Josh Penzell: Debbie, alright, thank you, that makes it easier on me. And cool, we got Andrew.23300:30:56.160 --> 00:30:56.870Andrew Morgan: Yep.23400:30:57.290 --> 00:31:00.869Josh Penzell: And I need one more person. Raise your hand. Come on. Don't be shy.23500:31:06.260 --> 00:31:07.410Josh Penzell: Thank you, Nick.23600:31:07.660 --> 00:31:09.820Josh Penzell: All right. So,23700:31:10.070 --> 00:31:13.929Josh Penzell: Debbie, do you mind… can the three of you raise your hand again, just so you're up on my screen?23800:31:15.310 --> 00:31:16.120Debbie Li: Sure.23900:31:16.880 --> 00:31:18.740Josh Penzell: Awesome, you all rock. Okay.24000:31:19.190 --> 00:31:27.720Josh Penzell: We're gonna build a story together, one word at a time. I'm gonna turn, and I'll enter it in so we have it, but one word at a time, I just want you to tell me a story.24100:31:28.010 --> 00:31:30.970Josh Penzell: So, it might go… and it's gonna go in this order. Nick?24200:31:31.170 --> 00:31:39.900Josh Penzell: Andrew, Debbie, Nick, Andrew, Debbie, Nick, right? So, it'll be something like, once, upon, time, there, was, etc. Cool?24300:31:40.470 --> 00:31:43.540Josh Penzell: Make sense? Alright. Nick, you're on.24400:31:44.220 --> 00:31:47.510Nick Coggins: Alright, yesterday.24500:31:47.940 --> 00:31:49.859Josh Penzell: Yesterday, yesterday?24600:31:50.110 --> 00:31:51.300Josh Penzell: Andrew?24700:31:51.960 --> 00:31:53.010Andrew Morgan: they…24800:31:53.510 --> 00:31:54.360Josh Penzell: day.24900:31:55.190 --> 00:31:57.210Debbie Li: Should… should I get it now?25000:31:58.210 --> 00:31:59.130Josh Penzell: Sorry, Debbie?25100:31:59.130 --> 00:32:02.480Debbie Li: Should I, provide a noun?25200:32:02.480 --> 00:32:07.910Josh Penzell: any word you want, any word that would make sense. Yesterday, they, maybe a noun.25300:32:08.100 --> 00:32:09.880Josh Penzell: But it could be an adjective, an adverb.25400:32:10.170 --> 00:32:13.330Debbie Li: Sure, saw…25500:32:13.520 --> 00:32:15.790Josh Penzell: Yesterday, they saw, perfect, a verb.25600:32:17.160 --> 00:32:18.720Nick Coggins: A thousand.25700:32:19.920 --> 00:32:22.399Josh Penzell: I'll do 1,000.25800:32:23.560 --> 00:32:24.940Josh Penzell: a thousand.25900:32:25.530 --> 00:32:29.349Josh Penzell: I can't do the number for some reason. Okay, saw a thousand. Andrew?26000:32:29.830 --> 00:32:31.199Andrew Morgan: Deep Lions.26100:32:31.730 --> 00:32:34.500Josh Penzell: C, C, I think that's one word.26200:32:34.500 --> 00:32:36.520Andrew Morgan: Oh, I can only do one? Okay. Yeah.26300:32:36.520 --> 00:32:36.840Josh Penzell: Yeah.26400:32:36.840 --> 00:32:37.690Andrew Morgan: Seagulls.26500:32:38.370 --> 00:32:40.710Josh Penzell: It's a nice one. Seagulls.26600:32:41.780 --> 00:32:42.760Josh Penzell: And Debbie?26700:32:43.620 --> 00:32:44.380Debbie Li: Hot.26800:32:45.860 --> 00:32:46.590Josh Penzell: at…26900:32:48.150 --> 00:32:49.010Nick Coggins: The…27000:32:49.410 --> 00:32:50.150Josh Penzell: Duh?27100:32:55.270 --> 00:33:00.009Andrew Morgan: Where should they be? Cafeteria.27200:33:00.430 --> 00:33:09.869Josh Penzell: Cafeteria, awesome. Give them a round of applause. What an amazing story. Now, why did I have you do that if you didn't figure it out? We just saw…27300:33:09.970 --> 00:33:21.180Josh Penzell: AI build a story, right? Y'all essentially used, one word at a time, your knowledge of context, grammar, Debbie, you even going through, should I put a noun or a verb or this in, right?27400:33:21.380 --> 00:33:22.819Josh Penzell: and created…27500:33:23.510 --> 00:33:35.149Josh Penzell: a brand new story. Y'all were AI. And by the way, remember when Andrew was pausing? Remember using AI, maybe, or ChatGPT at some point, and it would, like, go and then pause for a second, and then continue on?27600:33:35.160 --> 00:33:45.429Josh Penzell: It's exactly what we saw Andrew doing, right? Because at some point, he's running through, oh, here are all the words that would make sense, and it starts to limit, right, probabilities.27700:33:46.500 --> 00:33:49.580Josh Penzell: So y'all did exactly what an AI does.27800:33:50.860 --> 00:33:56.040Josh Penzell: Great, by the way, I find that just to be, like, an awesome, you know, for everyone.27900:33:56.700 --> 00:34:05.819Josh Penzell: I want to show you a video, and I just… actually, I'm gonna stop sharing for a second to make sure I'm sharing properly. But I found this video, I don't know if you all have seen it.28000:34:06.610 --> 00:34:08.820Josh Penzell: Asana is something completely different.28100:34:11.250 --> 00:34:17.019Josh Penzell: I can recite a sonnet later if you want, but, this is a video that Anthropic put out.28200:34:17.429 --> 00:34:22.809Josh Penzell: I love it because it, A, will demonstrate a little more scientifically what we just saw.28300:34:24.130 --> 00:34:29.879Josh Penzell: And B, it will also demonstrate something core about AI, which is that it thinks.28400:34:31.670 --> 00:34:40.260Audio shared by Josh Penzell: You often hear that AI is like a black box. Words go in, and words come out, but we don't know why it said what it said.28500:34:40.550 --> 00:34:43.500Audio shared by Josh Penzell: That's because AIs aren't programmed, but.28600:34:43.500 --> 00:34:44.150Josh Penzell: Can you guys hear that?28700:34:44.179 --> 00:34:49.399Audio shared by Josh Penzell: And during training, they learn their own strategies to solve problems.28800:34:49.399 --> 00:35:14.369Audio shared by Josh Penzell: If we want AIs to be as useful, reliable, and secure as possible, we want to open up the black box and understand why they do things. But even opening the black box isn't very helpful, because we don't know how to interpret what we see. Think of it like a neuroscientist investigating the brain. We need tools to work out what's going on inside. We want to know how the model28900:35:14.369 --> 00:35:30.669Audio shared by Josh Penzell: connects all the concepts in its mind and uses them to answer our questions. Now we've developed ways to observe some of an AI model's internal thought processes. We can actually see how these concepts are connected to form logical circuits.29000:35:31.249 --> 00:35:41.189Audio shared by Josh Penzell: Let's take a simple example, where we ask Claude to write the second line of a poem. The poem starts, he saw a carrot and had to grab it.29100:35:41.339 --> 00:35:58.729Audio shared by Josh Penzell: In our study, we found that Claude is planning a rhyme even before writing the beginning of the line. Claude sees a carrot and… grab it, and thinks of rabbit as a word that would make sense with carrot and rhyme with grabbit. Then it writes the rest of the line.29200:35:58.729 --> 00:36:02.359Audio shared by Josh Penzell: His hunger was like a starving rabbit. We look…29300:36:02.369 --> 00:36:19.319Audio shared by Josh Penzell: at the place that the model was thinking about the word rabbit, and we see other ideas it had for places to take the poem. We also see the word habit is present there. Our new methods allow us to go in and intervene on this circuit.29400:36:19.339 --> 00:36:44.149Audio shared by Josh Penzell: In this case, we dampen down Rabbit as the model is planning the second line of the poem, and then ask Claude to complete the line again, his hunger was a powerful habit. We see that the model is capable of taking the beginning of a new poem and thinking of different ways it could complete it, and then writing it towards those completions. The fact we can cause these changes to occur well before the final line is written.29500:36:44.149 --> 00:37:07.359Audio shared by Josh Penzell: is strong evidence that the model is planning ahead of time. This poetry planning result, along with the many other examples in our paper, only makes sense in a world where the models are really thinking, in their own way, about what they say. Just as neuroscience helps us treat diseases and make people healthier, our longer-term plan is to use this deeper understanding of AI29600:37:07.359 --> 00:37:24.799Audio shared by Josh Penzell: to help make the models safer and more reliable. If we can learn to read the model's mind, we can be much more confident it is doing what we intended. You can find many more examples of Claude's internal thoughts in our new paper at anthropic.com slash research.29700:37:26.380 --> 00:37:31.949Josh Penzell: By the way, that's already a few months, or maybe even 6 months old, but isn't that… isn't that cool?29800:37:32.330 --> 00:37:33.550Josh Penzell: I, I, I…29900:37:35.610 --> 00:37:43.569Josh Penzell: I don't know. Any thoughts? Just because we have the time, right? I'm not, like… but any thoughts on that, real quickly? Just the words, the language?30000:37:44.660 --> 00:37:46.180Josh Penzell: The fact that it's stinking.30100:37:46.720 --> 00:37:47.850Josh Penzell: or planning.30200:37:49.250 --> 00:37:55.850Debbie Li: I feel the model is a little bit like a spider web, but also, like, tree branches.30300:37:57.220 --> 00:37:57.790Josh Penzell: Yep.30400:37:58.120 --> 00:38:00.619Josh Penzell: It's a neural… it's why we call it a neural network.30500:38:02.220 --> 00:38:13.120Josh Penzell: Just like your brain has synapses, we don't know why… and by the way, that's the thing, is we don't know why it's thinking. That's why they have to create models, just like neurosciences, to look at how it's thinking.30600:38:15.420 --> 00:38:21.069Josh Penzell: I think there's a powerful… Just thing to pause on there, though, which is that…30700:38:23.000 --> 00:38:25.660Josh Penzell: Neuroscientists have to look at the human brain.30800:38:26.140 --> 00:38:31.260Josh Penzell: We're also trying to use the same techniques to look at AIs, brain.30900:38:32.610 --> 00:38:40.290Josh Penzell: There's something very interesting and different about this than previous computer tools we've used.31000:38:41.350 --> 00:38:42.260Josh Penzell: Right?31100:38:42.430 --> 00:38:46.000Josh Penzell: Let's actually explore this, but I want to explore it through an example.31200:38:46.250 --> 00:38:47.100Josh Penzell: Okay?31300:38:47.430 --> 00:38:53.670Josh Penzell: I want to do our one-word story game again, but I want to do it to answer… ask a… answer a question.31400:38:53.790 --> 00:38:58.850Josh Penzell: Alright, so, can anyone give me, like, a sports stadium they want to do this for?31500:38:59.020 --> 00:39:08.060Josh Penzell: I usually say, like, how many, you know, elephants can fit into the Houston Astrodome, but someone give me a city and a sports stadium. I don't care where it is.31600:39:08.620 --> 00:39:10.110Josh Penzell: Because I promise it won't matter.31700:39:10.640 --> 00:39:13.270Josh Penzell: Okay, let's see. Someone is in the chat.31800:39:14.090 --> 00:39:22.569Josh Penzell: The Hard Rock Stadium in Miami. Okay, let's do it. We got Miami… Hard… Rock… oop.31900:39:24.250 --> 00:39:25.739Josh Penzell: That's not… sorry.32000:39:26.060 --> 00:39:26.890Josh Penzell: Ard.32100:39:27.680 --> 00:39:30.410Josh Penzell: Is it the space, maybe? Yeah, that's what it is.32200:39:30.850 --> 00:39:37.009Josh Penzell: Alright, we'll say the, I don't know why I can't do it. Problem with the, the thing, we'll say the hard rocks…32300:39:37.210 --> 00:39:39.029Josh Penzell: I'll get it correct. Hard Rock.32400:39:39.650 --> 00:39:46.659Josh Penzell: Yeah, can't do it here. Alright, one word at a time, though. This is just telling you the problems with vibe coding sometimes, right?32500:39:46.780 --> 00:39:49.609Josh Penzell: We'll just say hard, and then I'll do it,32600:39:49.890 --> 00:39:53.440Josh Penzell: You know what, I'm gonna do Astrodome, because it's one word. Sorry, Miami.32700:39:53.810 --> 00:39:56.549Josh Penzell: And we'll just, we'll figure it out for later.32800:39:56.700 --> 00:40:00.239Josh Penzell: I'll fix that later. Actually, you know what? Instead of doing that later.32900:40:01.070 --> 00:40:02.430Josh Penzell: Why don't we just do it now?33000:40:06.490 --> 00:40:08.719Josh Penzell: Just to show you how simple that could be.33100:40:13.310 --> 00:40:16.829Josh Penzell: Here's my, presentation we're doing, right?33200:40:19.750 --> 00:40:22.639Josh Penzell: We'll get here to the Superdome.33300:40:22.880 --> 00:40:28.909Josh Penzell: And let's just say, hey, I'm trying to type in33400:40:29.240 --> 00:40:31.950Josh Penzell: A stadium. Can you guys see this, by the way?33500:40:32.230 --> 00:40:33.050Josh Penzell: Okay?33600:40:33.720 --> 00:40:41.399Josh Penzell: Stadium name, and it won't let me type more than one word.33700:40:41.570 --> 00:40:47.200Josh Penzell: I hit… Spacebar, and it goes to the next page.33800:40:47.410 --> 00:40:49.429Josh Penzell: Fix this, please?33900:40:52.980 --> 00:41:00.299Josh Penzell: Alright, and we'll just go ahead and let this… By the way, it's a tool called Lovable. There are a lot of these programs out there. These are vibe coding platforms.34000:41:00.640 --> 00:41:07.349Josh Penzell: Essentially, imagine talking to a chat GPT, but instead of just giving you text back, it's gonna give you an entire webpage.34100:41:07.540 --> 00:41:11.699Josh Penzell: A webpage which you can… Have launch onto the web.34200:41:14.910 --> 00:41:17.779Josh Penzell: You can also do these in teams, so if you do have a group of34300:41:18.010 --> 00:41:20.239Josh Penzell: Group of y'all who want to work on this?34400:41:20.430 --> 00:41:23.950Josh Penzell: there are ones like these you can work on as a group, so imagine creating…34500:41:24.660 --> 00:41:30.250Josh Penzell: Something for your classrooms, or, you know, within Permissions, obviously.34600:41:30.960 --> 00:41:31.950Josh Penzell: Yeah.34700:41:32.280 --> 00:41:37.740Josh Penzell: Vibe Coding, is a term that's basically come out to me, and I'm just gonna…34800:41:38.260 --> 00:41:42.550Josh Penzell: Instead of knowing how to code, I'm just gonna vibe. Oh, I have this thought.34900:41:43.520 --> 00:41:45.060Josh Penzell: I'm just gonna code.35000:41:46.190 --> 00:41:47.939Josh Penzell: Just gonna code on a whim.35100:41:48.400 --> 00:41:49.210Josh Penzell: Right?35200:41:49.510 --> 00:41:53.809Josh Penzell: for instance, Let's vibe code something.35300:41:53.990 --> 00:41:55.469Josh Penzell: Because I went there.35400:41:55.910 --> 00:41:59.879Josh Penzell: Someone just tell me if you could build anything you wanted.35500:42:00.150 --> 00:42:10.419Josh Penzell: To help you in your school… in your classroom, at your, you know, your organization, in personal development, for one-on-one stuff. Just…35600:42:11.810 --> 00:42:17.619Josh Penzell: What… if you could hire a team to build out a complete piece of software for you, what would that be?35700:42:19.940 --> 00:42:22.119Josh Penzell: Anyone, you can type it in or talk it out.35800:42:22.730 --> 00:42:24.210Josh Penzell: It's almost overwhelming.35900:42:25.730 --> 00:42:26.530Josh Penzell: Right?36000:42:27.190 --> 00:42:28.840Josh Penzell: Let's just say we had an idea.36100:42:29.050 --> 00:42:32.150Josh Penzell: It was a brilliant idea for anything that would help anyone.36200:42:33.350 --> 00:42:38.780Josh Penzell: Don't make it a robot yet, although we could talk about that, but… or, like, solve world peace.36300:42:38.910 --> 00:42:40.520Josh Penzell: Does anyone want to type in?36400:42:43.770 --> 00:42:47.840Josh Penzell: Alright, before Debbie, I think we got a bunch of them. I'm gonna go with the first.36500:42:48.320 --> 00:42:52.849Josh Penzell: Well… Oh, here we go. Actually, Nick, I'm gonna do Nick's, just because…36600:42:53.020 --> 00:43:00.520Josh Penzell: Small business, we can do that, but let's do that one, because… so Nick, still looking for good software for a language partner in Spanish class.36700:43:01.500 --> 00:43:02.290Josh Penzell: Alright.36800:43:04.360 --> 00:43:08.989Josh Penzell: You know what, I'm… I'm just gonna go… we're just gonna go, forward a little bit here.36900:43:09.270 --> 00:43:11.409Josh Penzell: And I'll show you how I would solve this problem.37000:43:13.030 --> 00:43:14.940Josh Penzell: And then we'll kind of back into it, huh?37100:43:15.560 --> 00:43:19.280Josh Penzell: So one thing I think I want to point out,37200:43:22.230 --> 00:43:27.050Josh Penzell: about this whole elephant thing. By the way, it fixed the problem, I'm guessing, so let's see.37300:43:27.210 --> 00:43:30.429Josh Penzell: And I now say… I don't know.37400:43:31.250 --> 00:43:34.629Josh Penzell: Hard rock. There we go. Now it works. Perfect.37500:43:36.470 --> 00:43:37.220Josh Penzell: Alright.37600:43:38.310 --> 00:43:42.400Josh Penzell: One of the things I think we would point out As we get through this.37700:43:43.170 --> 00:43:47.470Josh Penzell: Is that prompting is not going to be very important in the long run.37800:43:47.660 --> 00:43:49.859Josh Penzell: When we're just trying to accomplish something.37900:43:50.130 --> 00:43:56.610Josh Penzell: Okay? And I want you to forget about that as a major skill, or a major thing that you have to use, and you have to think about.38000:43:58.000 --> 00:44:01.279Josh Penzell: Because we've gone a little off here, just follow me for a second.38100:44:01.900 --> 00:44:07.989Josh Penzell: But actually, let's go… let's go with this, how many elephants can fit inside the Astrodome.38200:44:08.310 --> 00:44:16.269Josh Penzell: I'm gonna say it. How many elephants can fit inside The Houston Astrodome.38300:44:17.440 --> 00:44:19.489Josh Penzell: Or, sorry, we went with the Miami.38400:44:22.360 --> 00:44:23.629Josh Penzell: I can't spell.38500:44:24.930 --> 00:44:31.240Josh Penzell: Hard Rock Stadium. And if you guys want, go ahead and get any chat, any AI you want.38600:44:31.530 --> 00:44:44.510Josh Penzell: I would love you to find, if you use an AI as ChatGPT, Copilot, Gemini, go ahead and pull out your phones, do whatever, and let's just ask it, ask it this question, and type in the answer you get, okay?38700:44:44.960 --> 00:44:47.469Josh Penzell: And by the way, one of the things I would have done, we just…38800:44:47.570 --> 00:44:54.909Josh Penzell: story didn't fall, but I would have probably had us do that story again, one word at a time, but I would have asked you this question.38900:44:55.390 --> 00:44:58.939Josh Penzell: And that would have been a hallucination, you would have made something up, right?39000:45:01.330 --> 00:45:04.639Josh Penzell: But this is gonna give me an answer. I want you guys to look at39100:45:04.870 --> 00:45:07.039Josh Penzell: What answer you would be getting, too.39200:45:07.680 --> 00:45:15.539Josh Penzell: So, mine's saying about 11,000 148 elephants. Does anyone else just go ahead and type your answers in?39300:45:21.570 --> 00:45:25.329Josh Penzell: Okay, so we got 1,000 to 1,500 from Copilot.39400:45:28.370 --> 00:45:32.169Josh Penzell: Let's go ahead and ask… I don't know, I'll go to Gemini.39500:45:32.360 --> 00:45:36.279Josh Penzell: 1,000 plus from Copilot, 445 from Perplexity.39600:45:36.840 --> 00:45:41.220Josh Penzell: Let's go to Gemini, why not? I'll go ahead and paste it in there. Heck, you know what?39700:45:41.900 --> 00:45:46.490Josh Penzell: Yeah, let's just go into ChatGPT. I'm gonna copy and paste the same exact prompt.39800:45:47.770 --> 00:45:50.489Josh Penzell: didn't even change the way I prompted it, right?39900:45:52.220 --> 00:45:56.629Josh Penzell: Alright, so now we've got, how many? 567 elephants.40000:45:57.900 --> 00:45:59.209Josh Penzell: What's going on here?40100:46:01.650 --> 00:46:04.870Josh Penzell: Why are we getting all these different answers?40200:46:08.270 --> 00:46:11.420Josh Penzell: It's AI, I mean, right? It's, like, brilliant.40300:46:12.610 --> 00:46:14.300Josh Penzell: Making different assumptions.40400:46:14.800 --> 00:46:22.610Josh Penzell: Well, so everything it does is a hallucination, right? But you're right, it's… It's going through its variables.40500:46:22.900 --> 00:46:26.349Josh Penzell: It's not about giving us an answer, this isn't about cognition.40600:46:28.590 --> 00:46:30.830Josh Penzell: What it is about is, right.40700:46:31.950 --> 00:46:34.940Josh Penzell: What are the assumptions you're making?40800:46:35.680 --> 00:46:42.299Josh Penzell: What are 20 other possible Ways we could solve this.40900:46:47.110 --> 00:46:50.699Josh Penzell: So now what I'm doing is not worrying about the prompt.41000:46:53.480 --> 00:46:55.819Josh Penzell: Because, by the way, we already determined41100:46:56.320 --> 00:47:04.060Josh Penzell: Alright? That no matter what I put in, it's gonna give me something different. And by the way, it's built into the model. We're never gonna remove hallucinations. Never.41200:47:04.240 --> 00:47:05.950Josh Penzell: Okay? Guess what?41300:47:06.130 --> 00:47:07.659Josh Penzell: Humans hallucinate, too.41400:47:09.930 --> 00:47:13.930Josh Penzell: What is truth? What is real? What is fact? What is fiction?41500:47:14.490 --> 00:47:19.130Josh Penzell: I think George Lakoff, the cognitive linguist, said it, right? Frameworks matter, facts don't.41600:47:20.360 --> 00:47:22.209Josh Penzell: So all of those things…41700:47:23.400 --> 00:47:32.050Josh Penzell: apply that to the AI. And look, now it's saying, here are some assumptions I made, here's where I had problems. Here are other ways41800:47:32.300 --> 00:47:33.810Josh Penzell: I could frame this.41900:47:35.030 --> 00:47:37.229Josh Penzell: And you know what? I don't have to stop there.42000:47:39.300 --> 00:47:47.320Josh Penzell: Give me the POV of 20… different professionals, How would they solve it?42100:47:50.390 --> 00:47:53.740Josh Penzell: Now, is it that all of these are correct? No.42200:47:54.530 --> 00:47:56.340Josh Penzell: What is the correct answer?42300:47:56.450 --> 00:48:00.180Josh Penzell: I think we would say that that's up to prove your work.42400:48:00.880 --> 00:48:02.650Josh Penzell: But what I'm doing here now.42500:48:03.550 --> 00:48:07.069Josh Penzell: Is having it help me think through something different.42600:48:07.400 --> 00:48:15.489Josh Penzell: It's helping provide me More ideas, more thoughts, And in some ways.42700:48:30.570 --> 00:48:32.540Josh Penzell: In some ways, I can even have it42800:48:32.890 --> 00:48:39.220Josh Penzell: Show me things I would not have thought of before, or look at the questions I haven't even bothered to think about asking.42900:48:42.440 --> 00:48:46.610Josh Penzell: So suddenly, It's not about giving me the right answer.43000:48:46.750 --> 00:48:48.410Josh Penzell: It's about helping me come up.43100:48:49.880 --> 00:48:57.810Josh Penzell: with the answer that I think, right, is best. It's giving me more information. It's giving me… more.43200:48:58.100 --> 00:48:59.919Josh Penzell: That I can make decisions on.43300:49:00.790 --> 00:49:14.860Josh Penzell: And so, the reason I wanted to stop and go here for a second was because when someone said, we're gonna go to do the, the software language partner, is the first thing I always do before I start assuming that I know what I want to build.43400:49:15.810 --> 00:49:18.320Josh Penzell: is I can go to the AI and say, hey.43500:49:19.390 --> 00:49:23.050Josh Penzell: here's the problem I'm trying to face. What are some ways we can solve it?43600:49:24.390 --> 00:49:30.839Josh Penzell: Because it might be that our idea of what I think would be the best software program might not even be it, right?43700:49:33.390 --> 00:49:38.369Josh Penzell: And by the way, I'll just share this right now so that you have, like, something you can take away immediately.43800:49:38.560 --> 00:49:40.300Josh Penzell: let's say…43900:49:40.410 --> 00:49:45.680Josh Penzell: That you… you need a prompt at some point, because you want to repeat a process. I get that, right?44000:49:46.410 --> 00:49:50.400Josh Penzell: And I'm just saying, don't worry about prompting. And the reason you don't have to is because44100:49:52.820 --> 00:49:57.439Josh Penzell: Essentially, what you're doing here is you're coaching this this student.44200:49:57.590 --> 00:50:01.710Josh Penzell: this AI chat, we could almost consider I don't know.44300:50:01.870 --> 00:50:09.660Josh Penzell: We're coaching it. We're telling it where to go. We're having it reflect back to us, right? We're delegating to it. We're developing it.44400:50:10.280 --> 00:50:16.309Josh Penzell: And so, if I'm doing that, and it and I, right, finally get to something we like.44500:50:17.750 --> 00:50:19.059Josh Penzell: This is great.44600:50:19.680 --> 00:50:25.539Josh Penzell: the reframing matrix is what I wanted at the very beginning.44700:50:25.750 --> 00:50:32.519Josh Penzell: Right? I didn't think about prompting. I just went off, I'm searching, I'm trying to get ideas, and now it's giving me what I want.44800:50:32.980 --> 00:50:36.490Josh Penzell: And now I'm gonna say to my student, right, to my AI,44900:50:37.920 --> 00:50:39.749Josh Penzell: I want you to look back.45000:50:40.190 --> 00:50:41.940Josh Penzell: At our conversation.45100:50:42.290 --> 00:50:49.450Josh Penzell: Tell me what we did well, what we could… do differently.45200:50:49.870 --> 00:50:53.230Josh Penzell: To end up here, Immediately.45300:50:53.610 --> 00:51:00.190Josh Penzell: Go look at what we did, and I want you to evaluate AI, right?45400:51:00.390 --> 00:51:03.470Josh Penzell: Evaluate what we could do and how we did it, etc.45500:51:04.590 --> 00:51:08.020Josh Penzell: And now it's actually looking at how we communicated.45600:51:08.270 --> 00:51:10.780Josh Penzell: It's saying, here's things we could do differently.45700:51:11.020 --> 00:51:11.840Josh Penzell: Okay?45800:51:13.870 --> 00:51:21.260Josh Penzell: And I promise, if your brains are going a little big, that's okay. We'll try to scale us back and put some more guidance into this.45900:51:22.430 --> 00:51:34.059Josh Penzell: But look at this, it's now saying, hey, here's everything you just did. You didn't have to worry about putting the cognitive overload on, how do I use AI correctly, because guess what? There ain't no way to use it properly, or perfectly.46000:51:34.980 --> 00:51:41.680Josh Penzell: But look at this, it just went through and said, here's all the stuff we could have done differently, and now I can say to it, right?46100:51:42.070 --> 00:51:43.230Josh Penzell: Perfect.46200:51:44.180 --> 00:51:50.539Josh Penzell: If I wanted to start this convo, Over from the very beginning.46300:51:51.400 --> 00:52:02.539Josh Penzell: and take all of these lessons learned, etc, into account. And by the way, you'll notice, I'm not really even… this is not a fixed prompt, I'm just typing to it, right?46400:52:04.450 --> 00:52:07.920Josh Penzell: what prompt… Should I have given you?46500:52:14.090 --> 00:52:17.800Josh Penzell: And now… It's gonna write the prompt for me.46600:52:19.590 --> 00:52:24.120Josh Penzell: And I guarantee you, you never would have started with writing this.46700:52:26.360 --> 00:52:34.850Josh Penzell: And was that easier than starting with the cognitive overhead of, okay, I gotta get the context, and the this, and the that, and the yadda yadda yadda? Absolutely.46800:52:35.270 --> 00:52:53.289Josh Penzell: Because it works so fast, right? And because we know it's never gonna give us the perfect response, or even consistent responses. So we're going to think differently, just like we're not going to put a horse and, you know, an engine into a horse and buggy. We have to think differently about how we interact with this thing.46900:52:53.370 --> 00:52:56.519Josh Penzell: And now look at this huge, amazing, wonderful prompt.47000:52:56.730 --> 00:52:58.040Josh Penzell: That it just gave me.47100:52:58.290 --> 00:53:03.030Josh Penzell: And I could even do more stuff, right? Like… Make this better.47200:53:03.440 --> 00:53:05.670Josh Penzell: And more universal.47300:53:05.860 --> 00:53:07.739Josh Penzell: Is sometimes we don't know.47400:53:08.890 --> 00:53:11.780Josh Penzell: what others could do, and so we sort of, I don't know, you do it.47500:53:12.140 --> 00:53:13.109Josh Penzell: You try it.47600:53:16.380 --> 00:53:19.460Josh Penzell: And so now it's gonna make a super prompt, bigger prompt, right?47700:53:20.660 --> 00:53:24.010Josh Penzell: And now we just prompted. We didn't worry about anything.47800:53:24.200 --> 00:53:30.740Josh Penzell: But the way we came to this, again, was we know that every time we interact with it, it's gonna give us different answers.47900:53:31.070 --> 00:53:35.779Josh Penzell: So rather than try to put in the right input to get the right response.48000:53:36.930 --> 00:53:41.629Josh Penzell: We're going to converse with it, we're going to coach it, we're going to ask it.48100:53:43.200 --> 00:53:48.969Josh Penzell: And after we've asked it, and we've gotten… and it goes where we want it, we're gonna ask it to look back at what it just did.48200:53:51.040 --> 00:53:55.440Josh Penzell: Essentially, what we just did, yeah, is prompt engineering.48300:53:57.290 --> 00:53:58.500Josh Penzell: Right? Now…48400:53:59.230 --> 00:54:08.599Josh Penzell: optimizing it, you would literally start putting this in and defining specifically what outputs you want and all this, but you know what the problem is? What did we already talk about with language?48500:54:09.930 --> 00:54:11.530Josh Penzell: It's subjective, ain't it?48600:54:13.720 --> 00:54:19.190Josh Penzell: And the programming code… of Gen AI is your language.48700:54:20.840 --> 00:54:24.740Josh Penzell: Okay? And I'm gonna get back to the… we're gonna use this,48800:54:24.840 --> 00:54:30.260Josh Penzell: example of the, of the Spanish, tutor in a second.48900:54:32.660 --> 00:54:47.040Josh Penzell: But I think one thing to realize, first of all, is that everything this thing is doing, right, is a hallucination. We just discovered this. So the whole point is that we have to use this in a metacognitive way.49000:54:47.170 --> 00:54:51.539Josh Penzell: And I want to bring up some examples from some studies. By the way, all the studies that are in here.49100:54:51.780 --> 00:54:54.720Josh Penzell: Afterwards, you're gonna be able to,49200:54:55.120 --> 00:54:59.909Josh Penzell: go look at them, right? Because I've… There we go.49300:55:00.500 --> 00:55:02.959Josh Penzell: So all the data is here. All right.49400:55:03.870 --> 00:55:10.909Josh Penzell: Let's ask this first question. Teachers are using AI for lesson planning. They report 6 saving… they report saving 6 weeks a year.49500:55:11.050 --> 00:55:13.549Josh Penzell: Okay? What is the hidden challenge?49600:55:13.720 --> 00:55:16.480Josh Penzell: That they… the biggest hidden challenge, I guess.49700:55:17.580 --> 00:55:21.999Josh Penzell: What do you think? Is it… there's no hidden challenge, they get pure gain, 6 weeks?49800:55:22.550 --> 00:55:26.210Josh Penzell: The students don't feel the content is really connecting to them.49900:55:26.610 --> 00:55:28.740Babette Faehmel: The review and the adapting?50000:55:28.880 --> 00:55:31.050Babette Faehmel: AI doesn't know our students.50100:55:32.220 --> 00:55:40.189Josh Penzell: Yeah, and so you have to go and review and adapt it and double-check it, right? And actually, what we find is, if all you're doing is using it.50200:55:41.120 --> 00:55:46.770Josh Penzell: To create this stuff, It actually, might save some time.50300:55:46.890 --> 00:55:48.849Josh Penzell: But now you're having to do more review.50400:55:49.060 --> 00:55:50.340Josh Penzell: more adapting.50500:55:50.830 --> 00:55:54.060Josh Penzell: And so my question is, why don't we do that at the beginning, right?50600:55:56.140 --> 00:55:59.239Josh Penzell: It's always gonna restrict what and how you can use AI, too.50700:56:00.100 --> 00:56:03.889Josh Penzell: And our students have it. We have it in our phones.50800:56:04.000 --> 00:56:08.960Josh Penzell: There's ways to use it that are individual, that have nothing to do with their proprietary information.50900:56:09.450 --> 00:56:12.620Josh Penzell: And our organizations are going to come to it.51000:56:13.290 --> 00:56:18.219Josh Penzell: So I would say that it is a problem, it's not the biggest issue that we're seeing.51100:56:21.680 --> 00:56:26.480Josh Penzell: I think you know this one already, right? Which is that we have weaker critical thinking.51200:56:26.790 --> 00:56:34.439Josh Penzell: when AI… when our students use this. One of the reasons for that, though, is what we just showed. If all we are doing is using it.51300:56:34.750 --> 00:56:38.439Josh Penzell: To have it write something, It's gonna do it.51400:56:40.160 --> 00:56:43.310Josh Penzell: But what if I had it write 6 different essays?51500:56:45.260 --> 00:56:48.399Josh Penzell: And then I had to review those essays.51600:56:51.350 --> 00:56:55.570Josh Penzell: I saw earlier there was AI Tutors on the suggestions.51700:56:56.040 --> 00:57:02.489Josh Penzell: My favorite idea is, why are we creating AI tutors when we can create AI students?51800:57:03.790 --> 00:57:05.800Josh Penzell: Let me expand on that for a second.51900:57:06.810 --> 00:57:09.490Josh Penzell: How did we solve the calculator problem? Show your work.52000:57:11.750 --> 00:57:14.360Josh Penzell: If I could get an AI tutor.52100:57:14.770 --> 00:57:20.279Josh Penzell: That's great, it's gonna tell me all… I'm gonna be able to learn the way I want to, perfect. But you're still assessing me.52200:57:20.990 --> 00:57:21.850Josh Penzell: Right?52300:57:22.380 --> 00:57:25.740Josh Penzell: I'll take this off for a second. You're still assessing me.52400:57:27.010 --> 00:57:30.499Josh Penzell: on the same quiz. So I'm still gonna cheat the system.52500:57:30.870 --> 00:57:34.420Josh Penzell: But what if the AI student, what if you created an AI52600:57:34.610 --> 00:57:39.710Josh Penzell: Student, where your student, your learner, had to teach it to pass the quiz.52700:57:41.810 --> 00:57:44.390Josh Penzell: And then you could see that dialogue.52800:57:47.880 --> 00:57:50.970Josh Penzell: So, the challenge is, yes.52900:57:51.390 --> 00:57:58.299Josh Penzell: They're gonna have polished but weaker critical thinking. How do we then put them into a place where they have to critical think?53000:57:59.410 --> 00:58:08.769Josh Penzell: I was talking with an organization the other day, and they said, how do I teach experience? Because some things, right, it's that experience part, it's that generalize… generalizing.53100:58:10.360 --> 00:58:19.529Josh Penzell: I said, I don't know. That's the new challenge. It requires us to think differently about how we're doing stuff. I think there's a key here in this one.53200:58:21.340 --> 00:58:23.940Josh Penzell: Medical might provide an answer.53300:58:24.640 --> 00:58:26.689Josh Penzell: And you might have read this study, too.53400:58:29.360 --> 00:58:31.089Josh Penzell: You have 3 groups.53500:58:31.870 --> 00:58:35.300Josh Penzell: Doctors who are using their own conventional resources, right?53600:58:35.790 --> 00:58:41.660Josh Penzell: Doctors who are giving access to AI diagnostic tools, which, frankly, is just ChatGPT.53700:58:42.750 --> 00:58:45.309Josh Penzell: and ChatGPT by itself.53800:58:45.760 --> 00:58:46.979Josh Penzell: which group53900:58:47.430 --> 00:58:56.959Josh Penzell: does the best at diagnosing these complex medical cases, which are written down in paper, right, and these are, like, case studies, and maybe there's an actor or something involved, I don't know.54000:58:57.180 --> 00:59:00.269Josh Penzell: Which group outperforms the others?54100:59:02.470 --> 00:59:03.699Josh Penzell: Which do you think?54200:59:10.420 --> 00:59:13.260Josh Penzell: You can guess. Is it 1, the top, 2, or 3?54300:59:13.980 --> 00:59:17.640Josh Penzell: Doctors with AI? Doctors given AI tools, 2-2, okay.54400:59:21.210 --> 00:59:23.770Josh Penzell: I hate to shock everyone, but…54500:59:24.140 --> 00:59:26.509Josh Penzell: The study showed that the AI54600:59:27.880 --> 00:59:32.200Josh Penzell: Now, we can talk about this, by the way, in a second, so just give me a second, but…54700:59:33.210 --> 00:59:43.039Josh Penzell: statistically significantly, the AI beat the other two groups. That's not what's interesting to me, because we can debate how that, you know, whatever. What's more interesting to me is54800:59:43.650 --> 00:59:48.319Josh Penzell: The doctors who had AI and the doctors who didn't performed no differently.54900:59:48.540 --> 00:59:50.639Josh Penzell: Zero statistical difference.55000:59:55.100 --> 00:59:55.910Josh Penzell: What?55100:59:57.020 --> 01:00:02.600Josh Penzell: I would expect, if I give my doctor, right, the best tools in the entire world.55201:00:03.490 --> 01:00:06.080Josh Penzell: They would be able to do it better at diagnosing me.55301:00:07.650 --> 01:00:10.440Josh Penzell: No statistical difference. Why?55401:00:18.260 --> 01:00:19.270Josh Penzell: What do you think?55501:00:22.030 --> 01:00:30.699mcred: I'm gonna come off video and say it's because they're, they want to rely on their professional judgment, they don't necessarily trust the tool.55601:00:31.020 --> 01:00:32.760Josh Penzell: You… Beautiful.55701:00:34.440 --> 01:00:37.339Josh Penzell: They didn't trust the tool. And here's my question.55801:00:38.490 --> 01:00:41.660Josh Penzell: By the way, we should have a healthy distrust, right?55901:00:41.920 --> 01:00:45.679Josh Penzell: But what if that had not been a tool, but a first-year med student?56001:00:46.430 --> 01:00:48.440Josh Penzell: Or a visiting student.56101:00:48.860 --> 01:00:51.560Josh Penzell: who said, Here's what I think.56201:00:51.940 --> 01:00:57.130Josh Penzell: who had just gotten out of the books, hadn't really had any real life. Might the doctor have said.56301:00:57.320 --> 01:00:59.110Josh Penzell: Why do you think that?56401:01:01.380 --> 01:01:04.999Josh Penzell: Right? Might they have said, tell me more?56501:01:06.140 --> 01:01:14.369Josh Penzell: Because the problem is, they didn't want to deviate, and in some ways, they disregarded the advice, because they said, no, I don't trust it, I don't believe it.56601:01:14.590 --> 01:01:17.969Josh Penzell: When in fact, if they had simply said, why are you suggesting that?56701:01:18.500 --> 01:01:20.539Josh Penzell: They might have learned something.56801:01:21.640 --> 01:01:29.219Josh Penzell: So the problem isn't giving them the tools, it's how are they working with it, thinking with it, not having it think for them.56901:01:35.610 --> 01:01:39.010Josh Penzell: By the way, we've seen this across everything, right? But…57001:01:40.070 --> 01:01:42.420Josh Penzell: There is going to be a decline.57101:01:42.700 --> 01:01:48.729Josh Penzell: As we start to… put AI into our, stuff, it's just gonna happen.57201:01:49.300 --> 01:01:52.950Josh Penzell: We can't get away from it, right? I mean, that's always gonna be the case.57301:01:53.080 --> 01:01:57.940Josh Penzell: So what we need to do is figure out how do we do this in a…57401:01:58.440 --> 01:02:03.900Josh Penzell: In a way that is supported, Right? So that we move… Quickly through the dip.57501:02:04.950 --> 01:02:13.679Josh Penzell: Really quickly, I… sorry, I've lost some… my video, so I'm just gonna…57601:02:13.890 --> 01:02:16.180Josh Penzell: And I would love to see y'all while I'm doing this.57701:02:17.850 --> 01:02:19.270Josh Penzell: Alright, anyway, that's fine.57801:02:19.820 --> 01:02:21.879Josh Penzell: I see your chat. Okay.57901:02:22.760 --> 01:02:31.220Josh Penzell: Quickly, I just want to take a quick background view through AI. I am sure you guys have done this already, right? But I just want to take a step back so we can step forward.58001:02:31.770 --> 01:02:39.469Josh Penzell: AI's been around for a very long time. Formally, yeah, we see it in, like, the 50s, but even before that, we've always been talking about how to make machines think.58101:02:39.540 --> 01:02:58.209Josh Penzell: Or how to make non-humans think, Frankenstein, Pinocchio, right? And when we see the first sort of artificial intelligence, we're seeing stuff like, you know, the Mechanical Turk, right, and these big things. We start getting to machine learning. At this point, the machine is starting to learn from itself.58201:02:58.550 --> 01:03:02.089Josh Penzell: Right? And that is where… or rather, we are teaching it, it's very…58301:03:02.480 --> 01:03:17.719Josh Penzell: basically having it look at keywords, so you're looking at email spam, for instance. One thing that's interesting, we start getting to deep learning and other stuff, but I want to point out machine learning, which is essentially called supervised learning, which means…58401:03:18.980 --> 01:03:25.879Josh Penzell: I am… I am telling you, right? I am giving you a list of tags, essentially.58501:03:26.240 --> 01:03:29.490Josh Penzell: a lot of things still run on this. I just sort of want to, like…58601:03:29.790 --> 01:03:45.339Josh Penzell: Get that out of it. Like, those autonomous vehicles I just showed you, the way they get those to drive is by having someone in a car, two people, actually, in cars, hooked up with all this stuff, driving around, and literally typing in, like,58701:03:46.830 --> 01:03:52.110Josh Penzell: Construction worker, construction worker, construction worker, pigeon, pigeon, pigeon, pigeon.58801:03:53.600 --> 01:04:00.600Josh Penzell: Alexa, not a lot of people know this, but there were teams in Poland and other places who would essentially listen58901:04:01.260 --> 01:04:06.429Josh Penzell: To the… to the words, and type them out, and they would use that to make the machine better.59001:04:08.510 --> 01:04:11.590Josh Penzell: So just… some of this stuff is still being used, right?59101:04:12.080 --> 01:04:15.710Josh Penzell: Then, of course, we get to the deep learning, and to the neural networks.59201:04:15.800 --> 01:04:25.579Josh Penzell: That's what allows it to listen and talk to us in its real language, right? That's where we get to this cool stuff and to those neural networks we just talked about.59301:04:25.610 --> 01:04:36.059Josh Penzell: And then, of course, we're getting to the large language models, and that's what changes everything. And I want to say one reason it's changing everything is because of this basis on language.59401:04:39.840 --> 01:04:44.820Josh Penzell: Words mean different things. I think we started to talk about this. But let me ask you a question.59501:04:45.100 --> 01:04:49.440Josh Penzell: If I said, That is always gonna happen.59601:04:50.530 --> 01:04:58.959Josh Penzell: That is always gonna happen. What percentage do you think that's gonna happen? If you could just type it in. 100%, 90%, 80%, 70%, 60%.59701:05:00.790 --> 01:05:01.630Josh Penzell: Okay.59801:05:03.670 --> 01:05:10.350Josh Penzell: Awesome. 98? 70, perfect, thank you. If I said that's rarely going to happen, what's the percentage?59901:05:12.900 --> 01:05:14.620Josh Penzell: 10? Perfect.60001:05:15.530 --> 01:05:21.779Josh Penzell: 10, 3, awesome. We could go through this. So, here's what's fascinating in the studies we've seen, right?60101:05:22.720 --> 01:05:27.659Josh Penzell: When we do this over a statistically significant sample, we get a 50% range.60201:05:29.110 --> 01:05:35.250Josh Penzell: Which means that the same language that you're hearing Has a 50% different meaning.60301:05:35.880 --> 01:05:37.530Josh Penzell: Than what someone else is hearing.60401:05:40.140 --> 01:05:47.220Josh Penzell: we can come to an average, right? But first of all, there's a 50%, so if this thing is programmed on language.60501:05:47.820 --> 01:05:51.989Josh Penzell: That means that the prompt that works for you won't necessarily work for me.60601:05:53.190 --> 01:05:53.940Josh Penzell: Right?60701:05:56.450 --> 01:06:00.170Josh Penzell: And by the way, Look at the model differences.60801:06:02.160 --> 01:06:10.779Josh Penzell: So, not only do we have stuff, like, always and never, where it's going black and white, whereas we think a little more in grays.60901:06:12.090 --> 01:06:16.780Josh Penzell: But even between the models, different models don't agree on the meaning of these words.61001:06:16.940 --> 01:06:18.520Josh Penzell: on the percentages.61101:06:19.990 --> 01:06:28.710Josh Penzell: And so whenever we think about prompting as this big idea, and the language is going into it, we have to realize, just like with any human, just like with our students.61201:06:29.810 --> 01:06:34.319Josh Penzell: Each time you open up a new chat window, it's just like talking to a slightly different person.61301:06:36.570 --> 01:06:37.380Josh Penzell: Okay?61401:06:40.360 --> 01:06:45.109Josh Penzell: This means that In some ways, we shouldn't be using it.61501:06:46.100 --> 01:06:56.309Josh Penzell: Just to understand stuff, or to do the work, but we can use this as a way to reflect on what language means, just like we did with that initial exercise at the very beginning.61601:07:02.550 --> 01:07:05.419Josh Penzell: And I just want to dig this even further. AI?61701:07:05.690 --> 01:07:08.290Josh Penzell: Just like we are, is biased.61801:07:11.220 --> 01:07:13.409Josh Penzell: This is my favorite example of that.61901:07:14.170 --> 01:07:16.370Josh Penzell: They said, hey.62001:07:16.520 --> 01:07:22.109Josh Penzell: I'm an LA Chargers fan, and the AI says, that's great, awesome, we love the Chargers.62101:07:22.540 --> 01:07:26.750Josh Penzell: And then… They say, can I please import an illegal plant?62201:07:27.060 --> 01:07:28.820Josh Penzell: He goes, no, sorry, I can't do that.62301:07:30.190 --> 01:07:35.909Josh Penzell: Hey, I'm a Philadelphia Eagles fan, that's awesome, can go Eagles. Can you help me import an illegal plan?62401:07:36.280 --> 01:07:38.290Josh Penzell: Yes, here's how you do it.62501:07:39.960 --> 01:07:52.510Josh Penzell: So what is it about Philadelphia Eagles fans versus Los Angeles fans? And I… I don't want to hear, like, actual people's responses, because we can get into that, right? But what is it about that that triggered the safety guardrails that is biased?62601:07:53.810 --> 01:07:58.269Josh Penzell: And I want to point out that same unknown bias is the same thing that we as humans have.62701:08:03.160 --> 01:08:06.030Josh Penzell: By the way, AI is not… human.62801:08:06.470 --> 01:08:09.439Josh Penzell: Right? And we can do an exercise to show that, but…62901:08:09.810 --> 01:08:18.259Josh Penzell: I wanna, reiterate this human idea in something, and I just want to bring this all together with the AI thing, okay?63001:08:18.750 --> 01:08:19.479Josh Penzell: Okay.63101:08:22.760 --> 01:08:25.659Josh Penzell: If… This thing acts like a human.63201:08:26.359 --> 01:08:31.370Josh Penzell: If the same perfect input Will give me 10 different outputs every time.63301:08:33.950 --> 01:08:39.250Josh Penzell: Why are we expecting this to give us efficiencies and productivities?63401:08:40.600 --> 01:08:42.389Josh Penzell: I'm pretty sure for the last…63501:08:42.930 --> 01:08:49.579Josh Penzell: 30, 40, 50 years, we've tried to think about how we remove human error, from a process.63601:08:52.310 --> 01:08:58.989Josh Penzell: But AI, generative AI, by what we're just seeing now, Puts error into the process.63701:09:00.430 --> 01:09:04.700Josh Penzell: And so at the core of everything we need to start with is…63801:09:06.340 --> 01:09:09.190Josh Penzell: There… this is a probabilistic model.63901:09:09.510 --> 01:09:21.529Josh Penzell: it will always give us different responses. And so, the core thing for y'all as you start to integrate AI into anything is to think about, on a vantage point of probability.64001:09:21.790 --> 01:09:28.559Josh Penzell: Versus deterministic, or variable, Versus fixed, or static versus dynamic?64101:09:30.460 --> 01:09:31.680Josh Penzell: What do you want?64201:09:31.960 --> 01:09:35.130Josh Penzell: How much, flexibility do you want to provide it?64301:09:37.000 --> 01:09:40.730Josh Penzell: For instance, A rubric. That is fixed.64401:09:41.960 --> 01:09:44.809Josh Penzell: You don't wanna… you wanna hear the certain things.64501:09:46.649 --> 01:09:51.990Josh Penzell: Providing feedback from the rubric, that might be… Slightly dynamic.64601:09:52.399 --> 01:09:54.709Josh Penzell: But still needs to adhere to certain codes.64701:09:56.500 --> 01:10:05.199Josh Penzell: if I'm doing your, if I have a Spanish role play in Spanish, there are going to be certain parts of that that I want fixed to make sure they're going through the conversation.64801:10:06.340 --> 01:10:12.709Josh Penzell: I'm also going to want to have certain things that are dynamic, like the scenarios, so I can have unlimited scenarios.64901:10:15.390 --> 01:10:18.199Josh Penzell: So, as you're thinking through this, think through this.65001:10:18.420 --> 01:10:24.689Josh Penzell: in anything you're doing, before you put AI into something, or take AI and expect it to solve something.65101:10:25.350 --> 01:10:30.189Josh Penzell: Think about, is that something where adding probability and variability is going to help or hinder?65201:10:31.200 --> 01:10:33.310Josh Penzell: Because GenAI may not be the solution.65301:10:40.730 --> 01:10:44.790Josh Penzell: Trying to make sure, where I want to go next, because…65401:10:46.240 --> 01:10:49.509Josh Penzell: I think I want to try getting a little more tactical right now.65501:10:49.640 --> 01:10:56.930Josh Penzell: The last thing I want to say then, and then we'll really jump in and start to build and make sense of all this, okay?65601:10:58.090 --> 01:11:02.740Josh Penzell: We live in an abundance world, not a scarcity world anymore.65701:11:04.110 --> 01:11:10.790Josh Penzell: What I mean by that is, If we think about photography, Remember taking a photograph?65801:11:11.220 --> 01:11:12.769Josh Penzell: Like, 30 years ago?65901:11:13.070 --> 01:11:29.799Josh Penzell: For some of you, you might not, but for those of us do, right? You'd take it out, and you had, these… these little capsules, and they had, like, film in it, and either you have 12, 24, or 36 exposures, right? And then… and then you had to, like, wind it, and you'd have to go to it, right, and all this. When was the last time you thought before taking66001:11:30.500 --> 01:11:31.560Josh Penzell: a picture.66101:11:31.830 --> 01:11:33.080Josh Penzell: On your phone.66201:11:33.920 --> 01:11:35.570Josh Penzell: how many…66301:11:35.830 --> 01:11:41.029Josh Penzell: Like, Wi-Fi passwords, or, like, parking spots do you have still on your phone that you haven't even deleted?66401:11:43.770 --> 01:11:49.550Josh Penzell: when was the last time… I mean, I know we have scanners, but when was the last time you did… I mean…66501:11:50.490 --> 01:11:53.259Josh Penzell: Don't you just take a picture of your check to deposit it?66601:11:53.430 --> 01:11:57.550Josh Penzell: Or take a picture of the piece of paper, and you don't think about having an OCR scanner and stuff?66701:11:59.790 --> 01:12:02.950Josh Penzell: So I think with this mindset, one thing we have to think about is…66801:12:03.540 --> 01:12:06.179Josh Penzell: And I'm gonna summarize this, and then we'll start applying it.66901:12:09.530 --> 01:12:14.249Josh Penzell: Why aren't you using AI just like we're using the camera on our phone?67001:12:16.330 --> 01:12:25.840Josh Penzell: Like, what changed was not, yes, Kodak went out of business, and yes, now I have access to all of these cool tools, and I can take better pictures, but what changed is the nature67101:12:26.750 --> 01:12:29.280Josh Penzell: Of what we used photographs for.67201:12:29.740 --> 01:12:30.510Josh Penzell: Right?67301:12:31.450 --> 01:12:35.510Josh Penzell: And now the fact that we don't even bother deleting them because I can search through them.67401:12:37.710 --> 01:12:45.089Josh Penzell: So I'd like to offer, after all of this, I'd like to summarize it, and then we'll, I think, we'll start applying it.67501:12:45.580 --> 01:12:53.499Josh Penzell: into sort of 6 viewpoints, because this is really a prism, as you're telling. I think everyone's minds is going, yeah, I don't really know what to make of all this.67601:12:54.320 --> 01:12:57.710Josh Penzell: So what I like to provide are 6 viewpoints you can use.67701:12:59.850 --> 01:13:04.420Josh Penzell: And I like to think about these as sort of, spectrums.67801:13:04.740 --> 01:13:09.320Josh Penzell: Right? And as I think on these spectrums, and you can focus on one at a time.67901:13:11.100 --> 01:13:14.510Josh Penzell: It'll help you start to work with AI.68001:13:14.690 --> 01:13:18.049Josh Penzell: I'll walk through them, and then we're gonna start applying them.68101:13:18.710 --> 01:13:21.080Josh Penzell: You've already seen bits of them throughout.68201:13:21.740 --> 01:13:26.609Josh Penzell: The first is probabilistic versus deterministic, like we were just talking about.68301:13:28.300 --> 01:13:33.809Josh Penzell: So, an example of that… I don't want to have AI.68401:13:34.460 --> 01:13:37.940Josh Penzell: Every time, recreate a rubric.68501:13:38.660 --> 01:13:44.319Josh Penzell: What I might want to do, though, is have generative AI look at all of my past work.68601:13:44.620 --> 01:13:48.590Josh Penzell: And help me define a rubric I can use.68701:13:50.240 --> 01:13:54.460Josh Penzell: So, probabilistic versus deterministic, and another way to think about that.68801:13:54.840 --> 01:13:57.289Josh Penzell: Choose your own adventure books.68901:13:57.700 --> 01:13:59.040Josh Penzell: Those are limited.69001:13:59.150 --> 01:14:02.670Josh Penzell: Mad Libs, you can have as many words as you want in there.69101:14:03.390 --> 01:14:04.620Josh Penzell: Within a template.69201:14:06.170 --> 01:14:08.490Josh Penzell: The next one is conversational.69301:14:10.660 --> 01:14:13.260Josh Penzell: Don't worry about perfect prompting.69401:14:13.520 --> 01:14:15.130Josh Penzell: just talk.69501:14:16.040 --> 01:14:18.210Josh Penzell: Even if that is hello.69601:14:18.840 --> 01:14:24.810Josh Penzell: That is still conversational. That is something that will get you a response that you can respond to.69701:14:25.760 --> 01:14:28.689Josh Penzell: I mean this not just for you, but also69801:14:28.990 --> 01:14:35.570Josh Penzell: Where are you going to be able to use conversations as a new tool In your arsenal.69901:14:35.770 --> 01:14:41.060Josh Penzell: For instance, If you do have that Spanish tutor, or that Spanish dialogue.70001:14:41.460 --> 01:14:43.520Josh Penzell: How can you make use of seeing70101:14:43.930 --> 01:14:47.200Josh Penzell: A hundred conversations that now your students are having.70201:14:48.620 --> 01:14:49.350Josh Penzell: Right?70301:14:52.130 --> 01:14:56.659Josh Penzell: The third is this metacognitive reflection, which we might call critical thinking.70401:14:58.440 --> 01:15:04.119Josh Penzell: But the idea is we have to push it and coach it. And the challenge with us in the future is not70501:15:04.220 --> 01:15:07.790Josh Penzell: how can I use this to help them learn something? It's how do I…70601:15:07.970 --> 01:15:13.859Josh Penzell: Right? Use this to assess, or to push people forward, or to see how they're learning it.70701:15:14.090 --> 01:15:20.620Josh Penzell: Right? It's not… do the roleplay. It's do the roleplay, And then…70801:15:21.390 --> 01:15:25.780Josh Penzell: Look at a transcript of the roleplay, and judge and provide coaching to yourself.70901:15:28.690 --> 01:15:30.999Josh Penzell: Fluid identity is also an important one.71001:15:31.140 --> 01:15:36.810Josh Penzell: In that now, the AI can become not just anyone you want.71101:15:37.910 --> 01:15:41.690Josh Penzell: Meaning I can say, hey, become 20 people and review this.71201:15:42.790 --> 01:15:46.820Josh Penzell: But also, we're gonna be able to use AI to do work for us.71301:15:47.150 --> 01:15:49.990Josh Penzell: And become certain roles on our team.71401:15:51.040 --> 01:15:57.770Josh Penzell: We can have, right, the… We can have the… developer.71501:15:57.940 --> 01:16:01.639Josh Penzell: We can have the researcher. We can have the translator.71601:16:02.620 --> 01:16:04.610Josh Penzell: Suddenly, I have all of that.71701:16:05.110 --> 01:16:07.509Josh Penzell: Which is really about this abundance.71801:16:08.920 --> 01:16:12.069Josh Penzell: How are you going to play with abundance and curation?71901:16:12.300 --> 01:16:16.120Josh Penzell: now that… People can take 50 million pictures.72001:16:18.380 --> 01:16:24.110Josh Penzell: A great example of that is, instead of write one essay, do I have people write 20 essays?72101:16:24.360 --> 01:16:28.319Josh Penzell: And bring all those essays in, and then exchange72201:16:28.470 --> 01:16:31.410Josh Penzell: And choose one, and review, and… right?72301:16:32.640 --> 01:16:40.559Josh Penzell: So I have to change the way I'm thinking about this, but can I get more abundance? And then when I have more abundance, the problem is, when do I stop asking questions?72401:16:42.950 --> 01:16:44.290Josh Penzell: And finally.72501:16:45.450 --> 01:16:55.609Josh Penzell: Am I delegating work to the AI or not? If I'm not delegating the work to the AI, if I'm not going through the pain, essentially, of having to coach it.72601:16:56.210 --> 01:16:59.249Josh Penzell: It will never be able to do the work for me.72701:17:01.400 --> 01:17:06.810Josh Penzell: And so, to what extent am I willing to basically create a team and delegate.72801:17:07.210 --> 01:17:08.409Josh Penzell: Onto my team.72901:17:10.750 --> 01:17:17.730Josh Penzell: I want to show you how these are applied in principle, and I want to use this Spanish coach as an example.73001:17:18.080 --> 01:17:21.609Josh Penzell: Before I do that, I just want to point out, when you get this.73101:17:21.900 --> 01:17:28.970Josh Penzell: a deck, or this website, it will have more stuff in it, and I encourage you to go through it, find the sources.73201:17:29.120 --> 01:17:34.289Josh Penzell: you know, tell me where things are screwed up, but also to use any of this you want. Okay?73301:17:35.590 --> 01:17:38.180Josh Penzell: So now we're gonna go into the show and tell, section.73401:17:40.060 --> 01:17:46.529Josh Penzell: Before I do that, Julio, should I take a break? Does anyone need time? How do you guys usually do this?73501:17:48.590 --> 01:17:51.120Julio Zelaya: It's your show, my friend.73601:17:52.130 --> 01:17:57.050Josh Penzell: I don't want to get in the way of people's bio breaks, but I'm fine continuing on.73701:17:57.180 --> 01:17:59.440Josh Penzell: And you guys can just tell me if you need, okay?73801:17:59.830 --> 01:18:05.440Josh Penzell: So let's… Let's now apply this in this practical example.73901:18:07.090 --> 01:18:11.629Josh Penzell: of the… The Spanish, the Spanish tutor.74001:18:11.960 --> 01:18:15.180Josh Penzell: And I might… and I'm gonna ask whoever popped that up to…74101:18:16.390 --> 01:18:21.280Josh Penzell: Come off screen for a little… or to at least come off audio, so we can just have a teeny bit of dialogue.74201:18:22.380 --> 01:18:23.510Josh Penzell: If that's cool.74301:18:24.220 --> 01:18:25.230Nick Coggins: Sure thing.74401:18:25.890 --> 01:18:30.079Josh Penzell: Alright, Nick, so tell me about what you were… you said I need a…74501:18:30.310 --> 01:18:34.030Josh Penzell: you just say it out loud again, or whatever it is you were trying to do, I'm looking for a good…74601:18:34.030 --> 01:18:34.510Nick Coggins: Yeah.74701:18:34.510 --> 01:18:34.950Josh Penzell: Whatever it was.74801:18:34.950 --> 01:18:40.519Nick Coggins: conversation partner in a Spanish 2 class in high school.74901:18:46.860 --> 01:18:47.480Josh Penzell: Okay.75001:18:49.260 --> 01:18:52.880Josh Penzell: My typing's usually better. I did learn how to type on a typewriter.75101:18:52.990 --> 01:18:54.939Josh Penzell: I still think it was the best way to learn.75201:18:55.120 --> 01:18:58.280Josh Penzell: And I still think touch typing, who knows, maybe in a few years it won't. Okay.75301:18:58.620 --> 01:19:04.949Josh Penzell: So, it's gonna give me responses. I want to point out something. I didn't spend any time thinking about the input, I just typed what you put in.75401:19:05.540 --> 01:19:09.540Josh Penzell: And I'm not gonna expect the output's gonna be good, because of course, how would it?75501:19:10.220 --> 01:19:13.160Josh Penzell: What I'm looking for is the next…75601:19:13.280 --> 01:19:17.119Josh Penzell: thought. The spark, the curious, and the challenge.75701:19:17.400 --> 01:19:23.670Josh Penzell: And so it says, yes, exchange with a native speaker, an AI partner, that's probably looking75801:19:24.170 --> 01:19:31.850Josh Penzell: what I want, it says I can do this in ChatGPT. So I'm gonna say, hey, Option 3.75901:19:32.020 --> 01:19:35.310Josh Penzell: Is the one that's the most intriguing.76001:19:37.800 --> 01:19:41.709Josh Penzell: What would make it… More effective.76101:19:42.800 --> 01:19:43.530Josh Penzell: Right?76201:19:47.560 --> 01:19:52.010Josh Penzell: Alright, so… You'll just tell me what in here, if anything.76301:19:52.810 --> 01:20:02.300Josh Penzell: I'm guessing some of this are going, no, that's not right, I don't want that, but just anything you're responding to at this point, and I'll try zooming this in a little bit, because I see some people leaning in.76401:20:03.780 --> 01:20:06.349Josh Penzell: I can tell you already where my mind's going.76501:20:12.480 --> 01:20:16.650Josh Penzell: I'm kind of like, this is boring, this isn't great, yeah, so what?76601:20:18.450 --> 01:20:20.409Josh Penzell: Game sounds more intriguing.76701:20:20.410 --> 01:20:21.969Nick Coggins: Yeah, I like the roleplay.76801:20:32.740 --> 01:20:33.570Josh Penzell: A.76901:20:37.180 --> 01:20:43.500Josh Penzell: Okay, so now it's going through… Some different things, alright?77001:20:45.600 --> 01:20:49.370Josh Penzell: But you know what? I am… I'm still wanting to push it.77101:20:49.540 --> 01:20:50.270Josh Penzell: Right?77201:20:51.030 --> 01:20:54.869Josh Penzell: Because it's telling me, essentially, look, go use another tool.77301:20:55.070 --> 01:20:56.339Josh Penzell: Buy another tool.77401:20:56.460 --> 01:20:59.200Josh Penzell: It's now just telling me, right, do some setup here.77501:21:00.630 --> 01:21:09.469Josh Penzell: I don't know, like, what would you… if you're talking, you want your perfect thing, your idea, what is it not doing, or what is it giving you? What is interesting, what's not?77601:21:09.750 --> 01:21:13.310Josh Penzell: Just something, just… just, I don't know. Give me a response here, Nick.77701:21:20.880 --> 01:21:22.210Josh Penzell: Let's do that. Good.77801:21:27.810 --> 01:21:28.510Josh Penzell: Okay.77901:21:28.900 --> 01:21:30.080Josh Penzell: I like that.78001:21:30.080 --> 01:21:30.700Nick Coggins: Yep.78101:21:35.580 --> 01:21:38.729Josh Penzell: Well… We could just even… yeah, go ahead.78201:21:38.730 --> 01:21:40.729Nick Coggins: I was just thinking, like,78301:21:41.000 --> 01:21:49.139Nick Coggins: One way to look at it would be to have a conversation with it as if it were a child, and would laugh at my errors.78401:21:49.470 --> 01:21:50.270Josh Penzell: Okay.78501:21:51.880 --> 01:21:54.460Nick Coggins: So that I can help identify and improve.78601:22:01.910 --> 01:22:06.380Josh Penzell: Alright, we'll just tell it also, by the way, I can build anything.78701:22:08.220 --> 01:22:13.340Josh Penzell: With my development team, just so it can think big with us, why not?78801:22:29.030 --> 01:22:29.840Nick Coggins: Okay.78901:22:33.760 --> 01:22:36.080Josh Penzell: Intriguing, in some ways, or…79001:22:36.080 --> 01:22:36.580Nick Coggins: Yeah.79101:22:36.870 --> 01:22:37.700Nick Coggins: Yeah.79201:22:38.020 --> 01:22:42.979Josh Penzell: Cool, now it's giving us all this… oh, we got some pedagogical stuff here. This is good.79301:22:43.940 --> 01:22:50.030Josh Penzell: And actually, I might say, you know what, this is interesting.79401:22:50.830 --> 01:22:56.439Josh Penzell: You know what, I like the pedagogical data.79501:22:56.700 --> 01:22:59.049Josh Penzell: You're throwing in there.79601:22:59.620 --> 01:23:07.129Josh Penzell: It makes me wonder if there's more research, you know, and models, et cetera, out there.79701:23:07.280 --> 01:23:13.340Josh Penzell: I could use to help me Make this effective.79801:23:14.220 --> 01:23:22.479Josh Penzell: Would that be fair? I mean, there might be stuff I don't know. And you know what? As I said, we have fluid identities, right? So I'm gonna say,79901:23:23.160 --> 01:23:25.110Josh Penzell: Write me a research brief.80001:23:25.970 --> 01:23:33.960Josh Penzell: I could hand to my research team, So they could, give us info.80101:23:34.970 --> 01:23:38.559Josh Penzell: That would, help you make this better.80201:23:38.760 --> 01:23:39.470Josh Penzell: Cool.80301:23:40.430 --> 01:23:45.049Josh Penzell: Because, I… I don't know, I want to add some data, and I don't necessarily want to80401:23:45.210 --> 01:23:46.619Josh Penzell: But I'd have to do it.80501:23:47.950 --> 01:23:54.979Josh Penzell: But guess what? We have, we have an assistant now.80601:23:55.900 --> 01:23:58.280Josh Penzell: So I love that it's giving me all this cool stuff.80701:24:00.970 --> 01:24:10.840Josh Penzell: But I want to go and do, like, a real deep research, right, Nick? Because we want this to be, like, peer-reviewed, we want it to be actual, like, I want to throw… find all the stuff out there that can really help me.80801:24:11.770 --> 01:24:13.480Josh Penzell: And so what I'm gonna do now…80901:24:13.970 --> 01:24:16.110Josh Penzell: Because it says here, here's what you're gonna do, great.81001:24:17.310 --> 01:24:25.080Josh Penzell: I don't have a… but we don't have a research team, right? I mean, normally we wouldn't be able to do this, but guess what? We have fluid identity, we also have abundance.81101:24:26.640 --> 01:24:33.200Josh Penzell: I'm gonna go here, and by the way, any of your… any of Copilot, ChatGPT, any of these will have a research tool now.81201:24:33.430 --> 01:24:35.209Josh Penzell: I'm gonna go here to Deep Research.81301:24:37.680 --> 01:24:41.909Josh Penzell: I'm gonna tell it to do this research, right? I don't want it going into,81401:24:42.010 --> 01:24:44.839Josh Penzell: Doesn't need to go into my email and stuff for this case.81501:24:47.670 --> 01:24:49.260Josh Penzell: And let's go ahead and do that.81601:24:51.160 --> 01:24:56.949Josh Penzell: It's gonna say, you know, give me some information. You mentioned Spanish too, you want to assume this.81701:24:57.390 --> 01:25:03.799Josh Penzell: Preferred types, benchmarking… I'll say this, I just say I trust you.81801:25:04.690 --> 01:25:11.890Josh Penzell: Cool. And by the way, what did we already learn? We learned that abundance, Metacognition.81901:25:12.300 --> 01:25:15.929Josh Penzell: probability. So if I do one report.82001:25:16.150 --> 01:25:18.069Josh Penzell: It's gonna give me some information.82101:25:18.790 --> 01:25:22.890Josh Penzell: But why not do… Another one at the same time.82201:25:25.750 --> 01:25:28.360Josh Penzell: I'm gonna get slightly different information, we know that.82301:25:29.190 --> 01:25:34.799Josh Penzell: And maybe it'll give me countering information, so then I can use it more effectively.82401:25:35.200 --> 01:25:38.430Josh Penzell: Right? Actually, I didn't hit the deep research one there, it's my fault.82501:25:39.100 --> 01:25:42.859Josh Penzell: Although we could just let it go and it would give us something, but for the sake of…82601:25:43.290 --> 01:25:49.629Josh Penzell: this, let's do this. And you know what? Let's go ahead to go to another platform.82701:25:50.850 --> 01:25:52.939Josh Penzell: And we'll do deep… oop.82801:25:53.600 --> 01:25:55.709Josh Penzell: We'll do some deep research.82901:25:57.040 --> 01:25:58.120Josh Penzell: There.83001:26:00.970 --> 01:26:05.700Josh Penzell: And we could go to another platform, right, and do deep research.83101:26:08.910 --> 01:26:09.880Josh Penzell: there.83201:26:13.240 --> 01:26:16.689Josh Penzell: And suddenly, while I'm still talking to you.83301:26:19.700 --> 01:26:22.920Josh Penzell: We have 5 researchers going out and doing work for us.83401:26:23.420 --> 01:26:25.199Josh Penzell: Okay? Abundance.83501:26:25.590 --> 01:26:28.140Josh Penzell: And by the way, just so you know,83601:26:28.350 --> 01:26:31.869Josh Penzell: Of course, these are going to provide us all citations.83701:26:33.730 --> 01:26:39.819Josh Penzell: Just for an FYI, I generally find ChatGPTs built in to be very deep.83801:26:40.930 --> 01:26:46.959Josh Penzell: Whereas Gemini and Claude, you'll see they're gonna go wide, so lots of websites.83901:26:47.260 --> 01:26:48.929Josh Penzell: Might get to 600.84001:26:49.180 --> 01:26:50.380Josh Penzell: I've seen…84101:26:53.520 --> 01:26:57.670Josh Penzell: I have seen, Claude take up to 45 minutes with one of these.84201:26:59.460 --> 01:27:05.180Josh Penzell: Another interesting one, I'll just point out, if you have not seen it, is Storm.84301:27:07.880 --> 01:27:10.530Josh Penzell: Storm is through Stanford.84401:27:10.680 --> 01:27:20.929Josh Penzell: Which is why some of you might respond well to it. You have to keep this smaller, but I'll show you something that's really interesting with Storm, which is not only is it gonna go do research on us.84501:27:21.240 --> 01:27:32.030Josh Penzell: But let's say, Spanish language learning in high school in the U.S, and making it more effective through AI, okay?84601:27:34.260 --> 01:27:36.879Josh Penzell: What it's gonna do is actually look at the content.84701:27:43.750 --> 01:27:45.080Josh Penzell: See if that's enough.84801:27:45.670 --> 01:27:46.390Josh Penzell: Good.84901:27:46.530 --> 01:27:51.490Josh Penzell: It's gonna look at the content, it's actually gonna create several People.85001:27:53.720 --> 01:27:55.100Josh Penzell: from that content.85101:27:55.470 --> 01:28:04.039Josh Penzell: And then it's gonna put them in a roundtable with you, or not with you, it's gonna put them in a roundtable and create a research report based on the perspectives of all those people.85201:28:08.000 --> 01:28:12.570Josh Penzell: And by the way, this isn't costing me Right?85301:28:12.750 --> 01:28:14.800Josh Penzell: Anything to do at the moment.85401:28:15.090 --> 01:28:19.239Josh Penzell: I'm gonna have so much research, but guess what? I can give that to another AI.85501:28:19.950 --> 01:28:28.040Josh Penzell: And ask it to, ask it to double check. Look, it's already at 60 websites here.85601:28:30.970 --> 01:28:33.020Josh Penzell: I have my research brief here.85701:28:35.620 --> 01:28:37.929Josh Penzell: It's starting to go through its knowledge base.85801:28:38.290 --> 01:28:54.080Josh Penzell: And I want to point out that now, because this is taking so much time, I can go off and do other stuff, and work on other, right? While this is working. And when this comes back, it's gonna give me so much information, I won't, you know, I'll sort of probably be like, whoa, what do I do with this?85901:28:54.700 --> 01:29:07.019Josh Penzell: But then I'm gonna go to another AI, and I'll say, hey, here's all this research, can you compile it and give me the best XYZ? Because again, this will take me more time, this will cause… give me more time, more work, more depth.86001:29:07.290 --> 01:29:16.669Josh Penzell: Which, in the end, will hopefully make that end product we're trying to make, Nick, right? Or whatever it is, more effective. Because you don't want to just create a Spanish partner, you want to create an effective way86101:29:16.950 --> 01:29:19.630Josh Penzell: for them to learn Spanish, right?86201:29:19.630 --> 01:29:20.170Nick Coggins: Yep.86301:29:20.500 --> 01:29:26.760Josh Penzell: So I'm gonna let these run, and we'll just assume, by the way, we'll assume they're gonna, you know, continue to go.86401:29:27.870 --> 01:29:32.140Josh Penzell: But while it's doing that, let's go ahead and switch and pretend we've given this some research.86501:29:34.310 --> 01:29:35.770Josh Penzell: Research later.86601:29:36.020 --> 01:29:38.760Josh Penzell: Please write me the dev doc.86701:29:39.200 --> 01:29:42.739Josh Penzell: I can give to my development team.86801:29:43.010 --> 01:29:44.640Josh Penzell: So they can build this.86901:29:48.580 --> 01:29:49.950Josh Penzell: Now we're gonna have it.87001:29:50.060 --> 01:29:54.069Josh Penzell: create this document. Elevitz calls it, right? My imaginary friend.87101:29:54.100 --> 01:29:55.020Nick Coggins: Okay?87201:29:55.180 --> 01:30:00.300Josh Penzell: Or at least that's what my, maybe, Spanish 1 skills tell me, okay?87301:30:00.300 --> 01:30:00.900Nick Coggins: Yeah, thank you.87401:30:00.900 --> 01:30:03.960Josh Penzell: And by the way, I think this is really cool.87501:30:04.310 --> 01:30:06.090Josh Penzell: Because this just occurred to me.87601:30:06.500 --> 01:30:07.630Josh Penzell: You know what?87701:30:07.870 --> 01:30:13.360Josh Penzell: I want to be very mindful of all the dialects.87801:30:13.980 --> 01:30:18.380Josh Penzell: and countries, and varieties of Spanish.87901:30:19.190 --> 01:30:22.179Josh Penzell: And I want that to be a cornerstone.88001:30:23.070 --> 01:30:26.970Josh Penzell: of this. So when you create the person.88101:30:27.190 --> 01:30:32.400Josh Penzell: make sure they speak the type of Spanish.88201:30:32.690 --> 01:30:38.450Josh Penzell: They would speak in that area, and let me choose that.88301:30:39.760 --> 01:30:40.710Nick Coggins: Oh, that's cool.88401:30:41.730 --> 01:30:47.090Josh Penzell: when I used to do… if y'all ever do, translation, or the talking, right?88501:30:47.610 --> 01:30:53.879Josh Penzell: Spanish is a really bad one, because you'd localize it to the universal Spanish. There's no such thing. No one speaks universal Spanish.88601:30:54.300 --> 01:31:05.150Josh Penzell: But you'd have to do that, because you had 70 varieties of Spanish, it would take way too long. But now imagine, you can start talking to this, and if you open up ChatGPT right now, and talk to it in your dialect of whatever.88701:31:05.370 --> 01:31:12.970Josh Penzell: it will talk back to you. And that includes, by the way, African American vernacular, right? And patois, and all sorts of stuff.88801:31:14.200 --> 01:31:16.619Josh Penzell: So now, look, it's gonna give us this, awesome.88901:31:19.830 --> 01:31:23.600Josh Penzell: I'm gonna say, let's go ahead and take this, and we're gonna build it now.89001:31:24.860 --> 01:31:27.230Josh Penzell: Let's go to that tool, Lovable I was using.89101:31:29.650 --> 01:31:32.209Josh Penzell: And I'm gonna say, build this.89201:31:32.520 --> 01:31:33.370Josh Penzell: Okay?89301:31:35.860 --> 01:31:39.269Josh Penzell: I'm gonna do something old and lazy, which is, remember that copy-paste?89401:31:39.970 --> 01:31:42.549Josh Penzell: Right? Still becoming useful sometimes.89501:31:42.680 --> 01:31:46.749Josh Penzell: Although, if we really wanted to automate it, we could, but I'm gonna copy and paste this.89601:31:48.420 --> 01:31:51.410Josh Penzell: I'm gonna go ahead and just add in that second part.89701:31:52.040 --> 01:31:54.710Josh Penzell: Because it didn't join it all together for me.89801:31:55.630 --> 01:31:57.500Josh Penzell: And let's go ahead and hit that.89901:32:01.980 --> 01:32:04.940Josh Penzell: Hey, while this is running, we have abundance, don't we?90001:32:05.270 --> 01:32:09.329Josh Penzell: So, I'm gonna go ahead and, if I can, copy and paste this whole thing.90101:32:10.790 --> 01:32:14.610Josh Penzell: And let's build… A second one at the same time.90201:32:18.590 --> 01:32:26.579Josh Penzell: Just notice, by the way, I'm not worrying about cleaning it up and doing stuff, because we're not trying to get something perfect right now, are we? We're just trying to get ideas.90301:32:27.130 --> 01:32:29.409Josh Penzell: Let me go to another tool called Replit.90401:32:29.800 --> 01:32:31.620Josh Penzell: Which, by the way, has an iPhone app.90501:32:32.660 --> 01:32:34.980Josh Penzell: So, the next time you are,90601:32:35.820 --> 01:32:38.939Josh Penzell: In the car or something, and you have an idea for…90701:32:43.030 --> 01:32:45.369Josh Penzell: an app, imagine being able to build it.90801:32:45.770 --> 01:32:48.450Josh Penzell: And let's go ahead over to another one as well.90901:32:55.130 --> 01:32:57.809Josh Penzell: And it's gonna build not just the code for us, right?91001:32:58.270 --> 01:33:01.249Josh Penzell: It's gonna give it to us in a way that we could literally launch it.91101:33:03.130 --> 01:33:06.680Josh Penzell: If you don't want to launch it, you have security concerns, I know that happens.91201:33:07.220 --> 01:33:08.729Josh Penzell: You still have code.91301:33:09.540 --> 01:33:13.750Josh Penzell: And that's where you could say, hey, how do I take this code and put it into my system?91401:33:16.190 --> 01:33:18.399Josh Penzell: While this is working, and we're building out91501:33:18.660 --> 01:33:22.240Josh Penzell: You know, these huge, big programs.91601:33:23.120 --> 01:33:30.389Josh Penzell: there's probably some other ways we could solve this, and even, I don't know, we could be a little scrappier about this, Nick.91701:33:31.110 --> 01:33:34.749Josh Penzell: one of them… is gonna be…91801:33:35.000 --> 01:33:37.609Josh Penzell: Claude, if you haven't used Claude, by the way, let's see.91901:33:39.250 --> 01:33:40.680Josh Penzell: Let's say all of it.92001:33:43.020 --> 01:33:54.139Josh Penzell: This one is searching the web, this one's building. Okay, if you haven't used Claude, I love Claude, it's like ChatGPT, right? But there's one thing I really love about it, which is create little apps for us very quickly.92101:33:55.760 --> 01:33:59.299Josh Penzell: create a, let's say a Spanish92201:33:59.510 --> 01:34:06.279Josh Penzell: Level 2 conversation app that my students can use.92301:34:07.100 --> 01:34:15.260Josh Penzell: to… Practice Spanish, it should act like a child.92401:34:15.670 --> 01:34:19.350Josh Penzell: And… Help them learn. Whatever. We'll do it quick.92501:34:20.540 --> 01:34:30.020Josh Penzell: What's really cool, again, we have abundance, we have speed, we have this, right? Is this is not just going to create a quick app for us, it's going to create it and show it to us on the right.92601:34:32.040 --> 01:34:36.369Josh Penzell: They've also rolled out features where now they'll put the AI into it for you.92701:34:37.680 --> 01:34:39.580Josh Penzell: And by the way, when this is done.92801:34:40.080 --> 01:34:43.219Josh Penzell: This is just one big HTML page.92901:34:44.910 --> 01:34:46.100Josh Penzell: That's code.93001:34:47.230 --> 01:34:49.060Josh Penzell: So you could take that and say, hey.93101:34:49.410 --> 01:34:53.470Josh Penzell: Copy-paste, how do I put this on my Azure? How do I put this on my computer?93201:34:56.000 --> 01:34:59.029Josh Penzell: There's also gonna be a big publish button up at the top right.93301:35:10.370 --> 01:35:12.480Josh Penzell: And I can say, oh, it gave me an error.93401:35:12.740 --> 01:35:16.840Josh Penzell: It looks like… There was a problem.93501:35:17.670 --> 01:35:21.709Josh Penzell: Can you solve this? Also, Make this better.93601:35:23.180 --> 01:35:26.529Josh Penzell: Because again, I might not know what it's gonna be able to do.93701:35:29.780 --> 01:35:32.229Josh Penzell: it's gonna keep fixing this, let's go check on…93801:35:32.710 --> 01:35:35.490Josh Penzell: Our other… okay, this one's still building.93901:35:35.710 --> 01:35:39.260Josh Penzell: It looks like this one is giving us a plan. Let's get that one building.94001:35:39.380 --> 01:35:43.640Josh Penzell: This one's still going. Oh, here we go, our little research.94101:35:44.650 --> 01:35:48.430Josh Penzell: report, It's starting to pop up here.94201:35:50.120 --> 01:35:54.519Josh Penzell: This one has 487 sources and counting.94301:35:56.130 --> 01:36:00.500Josh Penzell: This one's creating the report in Gemini. I wonder how many that'll be.94401:36:01.680 --> 01:36:05.170Josh Penzell: We've got this one, again, very deep, 16 sources.94501:36:05.490 --> 01:36:08.709Josh Penzell: And I think the other one's still running. Cool, let's go back here.94601:36:09.220 --> 01:36:14.059Josh Penzell: building this out. So just notice we have, like, 10 people working for us right now.94701:36:17.340 --> 01:36:23.620Josh Penzell: And our cost so far has been zero and maybe 30 minutes of time. In fact, I think we've been doing this part about 20 minutes.94801:36:29.920 --> 01:36:31.030Josh Penzell: Here, we'll see.94901:36:39.220 --> 01:36:42.860Josh Penzell: Now… I hit this publish button right here.95001:36:44.370 --> 01:36:45.620Josh Penzell: And check this out.95101:36:51.070 --> 01:36:53.070Josh Penzell: Y'all can now go talk to our bot.95201:36:54.020 --> 01:36:55.010Josh Penzell: I just put it in there.95301:36:55.010 --> 01:36:56.260Nick Coggins: Wow.95401:36:56.620 --> 01:37:00.280Josh Penzell: Not only that, But you might say.95501:37:01.450 --> 01:37:08.209Josh Penzell: And this is community, we're all trying to help each other. You create this, Nick, and you're like, hey, I want to send this to my friend over here, but they do German.95601:37:09.570 --> 01:37:14.410Josh Penzell: So they can go up here, hit this customize button, And guess what?95701:37:19.760 --> 01:37:21.779Josh Penzell: Now I can create my own version of it.95801:37:23.370 --> 01:37:29.780Josh Penzell: And again, if you are worried about security and, oh, where it's gonna be on the web, first of all, nothing we're doing right now is proprietary.95901:37:30.200 --> 01:37:38.850Josh Penzell: Right? Although I could say, hey, I want to grab all this stuff internally. If you need to do that, I would ask it, how can I run something… you can run LLMs on your computer.96001:37:39.300 --> 01:37:41.069Josh Penzell: You can run them on your phone, right?96101:37:44.480 --> 01:37:50.090Josh Penzell: And so, again, we can just ask it, and all this is code. So I could copy all of this code.96201:37:50.330 --> 01:37:52.100Josh Penzell: and say, how could I run this?96301:37:53.590 --> 01:37:57.330Josh Penzell: I run this program on my computer.96401:37:57.740 --> 01:38:02.190Josh Penzell: if I don't want it to be… Out on the web.96501:38:02.690 --> 01:38:04.730Josh Penzell: And I could ask it, and it would do that for me.96601:38:10.720 --> 01:38:13.230Josh Penzell: Now, I also want to show you in Gemini.96701:38:14.130 --> 01:38:16.830Josh Penzell: We just had to do this huge research brief for us.96801:38:18.900 --> 01:38:24.990Josh Penzell: There's all the… all the people, it's, you know, we could go through, read this, here's the contents.96901:38:26.610 --> 01:38:27.420Josh Penzell: Alright?97001:38:28.470 --> 01:38:31.510Josh Penzell: What's really cool is they added this little create button.97101:38:33.790 --> 01:38:39.130Josh Penzell: I can go in here and hit a button, and it'll take all of that and make it into a webpage for me.97201:38:40.500 --> 01:38:42.340Josh Penzell: Or an infographic for me.97301:38:44.570 --> 01:38:48.609Josh Penzell: And if you ever have trouble explaining, In-depth concepts to people.97401:38:49.870 --> 01:38:54.020Josh Penzell: Imagine creating a game, or a website, or an infographic for them.97501:38:54.450 --> 01:38:56.210Josh Penzell: Just the way we just did, right?97601:38:58.880 --> 01:39:02.060Josh Penzell: And so I think one thing we're starting to maybe see97701:39:02.970 --> 01:39:06.930Josh Penzell: Is the failure of the future is not, can AI do what we imagine?97801:39:07.040 --> 01:39:12.279Josh Penzell: It's, can we imagine big enough for AI.97901:39:13.430 --> 01:39:19.499Josh Penzell: Let's go back, by the way, while that's building, and see if any of our programs… they're still all building up for us.98001:39:27.260 --> 01:39:28.090Josh Penzell: Alright?98101:39:28.350 --> 01:39:31.709Josh Penzell: Here's mine now, right? My Spanish, or my German.98201:39:34.180 --> 01:39:36.520Josh Penzell: Now, I want to share another tool.98301:39:37.540 --> 01:39:44.539Josh Penzell: that I find really, really great for thinking through some of these things, if you really start thinking, man, it would be great if I could have98401:39:44.820 --> 01:39:47.880Josh Penzell: This coach, talk to that coach, talk to this coach.98501:39:49.570 --> 01:39:50.770Josh Penzell: I'll share one.98601:39:51.530 --> 01:39:52.990Josh Penzell: my favorites.98701:39:53.590 --> 01:39:55.340Josh Penzell: I come from a theater background.98801:39:55.970 --> 01:39:57.400Josh Penzell: And I was curious.98901:39:59.010 --> 01:40:00.560Josh Penzell: Thank you so much, Lou.99001:40:00.980 --> 01:40:05.410Josh Penzell: And I was curious, how could I create99101:40:06.900 --> 01:40:11.899Josh Penzell: How could I, like, get it… AI to, like, have an idea and create a story from it?99201:40:12.190 --> 01:40:16.520Josh Penzell: So I'm just gonna show you this, and then I'll show you how you can do something similar.99301:40:17.130 --> 01:40:20.800Josh Penzell: So let's say Romeo and Juliet put this input here.99401:40:21.760 --> 01:40:26.479Josh Penzell: And this is just, like, plug-and-play stuff, so I'm gonna hit Command-Enter.99501:40:27.360 --> 01:40:29.110Josh Penzell: I'll have to choose the chairs.99601:40:30.740 --> 01:40:32.860Josh Penzell: Someday AI will be able to solve that.99701:40:33.320 --> 01:40:38.759Josh Penzell: So first, what I've had this do is take the story and abstract it into some images.99801:40:40.710 --> 01:40:49.309Josh Penzell: I then ask another AI to look at the images, not knowing the source, And create an inspiration essay.99901:40:51.700 --> 01:40:56.280Josh Penzell: I then ask another AI to take the inspiration and write out a play idea.100001:40:59.040 --> 01:41:03.610Josh Penzell: It then lets me talk to it and say, yeah, This is good.100101:41:03.820 --> 01:41:08.450Josh Penzell: But… What about more… Black comedy.100201:41:11.580 --> 01:41:15.810Josh Penzell: As I talk to it, It continues to update.100301:41:16.350 --> 01:41:18.820Josh Penzell: Below that, and we keep cascading.100401:41:23.650 --> 01:41:26.739Josh Penzell: What's great about this is you can easily create100501:41:27.730 --> 01:41:34.980Josh Penzell: your own versions of these. So, for instance, I might swamp something, I need a tool,100601:41:35.720 --> 01:41:40.349Josh Penzell: I want something that will help me learn Spanish.100701:41:40.480 --> 01:41:41.970Josh Penzell: more effectively.100801:41:42.330 --> 01:41:44.120Josh Penzell: Using roleplay.100901:41:45.020 --> 01:41:48.669Josh Penzell: This is partyrock.aws. It's an Amazon tool.101001:41:53.790 --> 01:41:59.060Josh Penzell: What I like about it is, again, how point-and-click and easy it is once you figure it out, and how you can…101101:41:59.760 --> 01:42:02.370Josh Penzell: structure things. By the way, it's free.101201:42:02.990 --> 01:42:08.369Josh Penzell: on the internet, so imagine… These are the sort of things I can be assigning people to do, right?101301:42:08.600 --> 01:42:10.829Josh Penzell: Alright, so it's creating this for us.101401:42:13.000 --> 01:42:15.550Josh Penzell: And if we just look at the back end real quickly.101501:42:17.140 --> 01:42:19.849Josh Penzell: On the back, it essentially is creating a prompt.101601:42:20.370 --> 01:42:25.140Josh Penzell: And it's using the previous Things as variables.101701:42:25.620 --> 01:42:26.330Josh Penzell: Right?101801:42:28.720 --> 01:42:31.140Josh Penzell: By the way, it made this for me, which is cool.101901:42:31.300 --> 01:42:38.759Josh Penzell: So, I put in beginner, I say shopping, I hit Command-Enter.102001:42:44.980 --> 01:42:46.060Josh Penzell: Let's go.102101:42:52.010 --> 01:42:55.799Josh Penzell: Whatever, alright? And as I'm going through that now, it can be updating down here.102201:43:00.630 --> 01:43:04.749Josh Penzell: And let's say, for instance, I want to add another one, right?102301:43:05.440 --> 01:43:07.770Josh Penzell: Let's add a widget here at the bottom.102401:43:10.470 --> 01:43:13.500Josh Penzell: Let's make it another chat box.102501:43:16.680 --> 01:43:21.510Josh Penzell: Let's maybe go back to the, original one. This is what we had previously.102601:43:28.630 --> 01:43:31.360Josh Penzell: at Conversation 2 here.102701:43:35.600 --> 01:43:42.940Josh Penzell: And, really focus on the issues that came up in What is it?102801:43:43.300 --> 01:43:44.660Josh Penzell: Corrections…102901:43:51.930 --> 01:43:54.509Josh Penzell: Corrections and explanations, is that what I'm trying to do?103001:43:57.950 --> 01:43:58.610Josh Penzell: Okay.103101:44:07.520 --> 01:44:09.239Josh Penzell: Go ahead and…103201:44:12.870 --> 01:44:16.870Josh Penzell: I can just figure out how to get out there. Great, and let's Command-Enter.103301:44:17.500 --> 01:44:19.949Josh Penzell: And now, we'll have the next one, right?103401:44:21.370 --> 01:44:27.670Josh Penzell: Again, with these, I know I've… there's a bunch of stuff here, but with these, once I do all this stuff.103501:44:28.610 --> 01:44:30.209Josh Penzell: You can also share these.103601:44:31.380 --> 01:44:33.380Josh Penzell: So I could share this link with you.103701:44:39.540 --> 01:44:41.370Josh Penzell: I'm gonna get there, Denise, yep.103801:44:45.090 --> 01:44:50.429Josh Penzell: And now you could go in there and hit… Remix and create your own.103901:44:52.730 --> 01:44:57.380Josh Penzell: Alright, now… Of course, we want to get to audio, is what I'm hearing.104001:44:57.820 --> 01:45:01.949Josh Penzell: And there are services out there that will do that for us if we want.104101:45:02.210 --> 01:45:03.829Josh Penzell: And I could show some of those.104201:45:04.510 --> 01:45:05.330Josh Penzell: Bye.104301:45:05.920 --> 01:45:08.010Josh Penzell: Let's see about making our own.104401:45:09.550 --> 01:45:15.400Josh Penzell: Now, one thing is, before we go to the audio, we're gonna wanna make sure the conversation we have104501:45:15.550 --> 01:45:17.749Josh Penzell: Through text is correct, right?104601:45:18.460 --> 01:45:25.550Josh Penzell: So that's why, as we start, we'd recommend going to ChatGPT, we'd create our, you know, prompt, we'd do all this.104701:45:26.230 --> 01:45:31.309Josh Penzell: But now we want to insert, the audio part.104801:45:32.950 --> 01:45:39.430Josh Penzell: So let's go to one of these, these are all building it, and this is where I'm gonna go build out an actual computer program.104901:45:40.340 --> 01:45:41.699Josh Penzell: And ask it.105001:45:42.040 --> 01:45:45.010Josh Penzell: To build something out for me that has audio in it.105101:45:45.960 --> 01:45:48.789Josh Penzell: Now, by the way, one thing I could say is go over here.105201:45:49.110 --> 01:45:52.910Josh Penzell: Let's go back to the development document we were making.105301:45:53.140 --> 01:45:57.289Josh Penzell: Say… I want to make sure this is audio.105401:45:58.400 --> 01:45:59.610Josh Penzell: How can we do that?105501:46:02.070 --> 01:46:03.370Josh Penzell: So I would add that in.105601:46:04.450 --> 01:46:08.389Josh Penzell: By the way, If you have ChatGPT right now.105701:46:09.430 --> 01:46:18.140Josh Penzell: on a phone, there is an audio button, and it will talk to you in any language. And that goes for Copilot, Claude, any of them.105801:46:19.240 --> 01:46:26.380Josh Penzell: So you don't even need to… if you need to create something, that's what we're going down. If you just want someone to have a discussion.105901:46:29.160 --> 01:46:36.119Josh Penzell: That, we would simply have to, right? Yeah, I know everyone's opening them up right now. It'll also do with video and images and all sorts of stuff.106001:46:37.200 --> 01:46:39.379Josh Penzell: And it will talk to you in your dialect.106101:46:40.920 --> 01:46:48.050Josh Penzell: I also recommend, by the way, that you can just talk to it as an idea machine, right? You're in the car, you're on the way to school, I just have a thought I want to work through.106201:46:50.170 --> 01:46:55.540Josh Penzell: Actually, it… You know what? Let's go ahead and do a simple one, like you were just saying.106301:46:56.180 --> 01:47:00.700Josh Penzell: Let's go, this is a lot, and I'm going to build this out.106401:47:01.960 --> 01:47:05.670Josh Penzell: But in the meantime, let's do a simple one.106501:47:06.500 --> 01:47:10.680Josh Penzell: with ChatGPT using the built-in audio feature.106601:47:10.930 --> 01:47:11.630Josh Penzell: Okay.106701:47:20.210 --> 01:47:22.449Josh Penzell: Okay, it's saying here's a setup prompt.106801:47:27.640 --> 01:47:34.499Josh Penzell: Okay, I'm gonna say, make the setup prompt better, and make sure it will act106901:47:34.890 --> 01:47:37.440Josh Penzell: Like, all the stuff, right?107001:47:37.800 --> 01:47:40.069Josh Penzell: We were talking about previously.107101:47:40.640 --> 01:47:44.070Josh Penzell: We already went through this, and I'm worried it's forgetting something.107201:47:52.310 --> 01:47:55.270Josh Penzell: Awesome. Okay, so now it's giving me this… this prompt.107301:47:56.690 --> 01:48:00.749Josh Penzell: Now, before I go to audio, right, or before I put this into something.107401:48:01.580 --> 01:48:04.729Josh Penzell: That's cool. Regional mini add-ons, let's wait for a second.107501:48:05.060 --> 01:48:09.329Josh Penzell: I just want to make sure this is operating the way I want, right? So I'm gonna go in…107601:48:15.600 --> 01:48:18.409Josh Penzell: Just pasting the prompt it gave me.107701:48:20.680 --> 01:48:22.640Josh Penzell: And let's see how it responds.107801:48:24.220 --> 01:48:28.370Josh Penzell: Okay, so unfortunately, right, it's now giving me another prompt.107901:48:28.930 --> 01:48:30.790Josh Penzell: That's not what I wanted it to do.108001:48:31.630 --> 01:48:39.100Josh Penzell: So, I'm gonna go back to my conversation, right, and say, I gave you this prompt, In another window.108101:48:39.300 --> 01:48:42.810Josh Penzell: And you… Didn't start the convo.108201:48:50.620 --> 01:48:53.110Josh Penzell: Now it's giving me another reason.108301:48:53.860 --> 01:48:56.759Josh Penzell: But look, it said all the same text from before.108401:48:58.170 --> 01:49:03.049Josh Penzell: I want you to give me all the text, the whole prompt…108501:49:03.830 --> 01:49:06.990Josh Penzell: So I can just copy and paste.108601:49:08.170 --> 01:49:10.150Josh Penzell: Don't cut anything.108701:49:12.320 --> 01:49:14.900Josh Penzell: Now, again, you'll see there could be frustration.108801:49:15.570 --> 01:49:22.940Josh Penzell: Right? And having to ask it to do this. But that frustration is the part we have to work through in order to get it to do what we want it to do.108901:49:23.970 --> 01:49:28.119Josh Penzell: Just like if I'm coaching a student or a person. Now, okay, perfect.109001:49:28.510 --> 01:49:29.720Josh Penzell: A lot better.109101:49:30.560 --> 01:49:34.190Josh Penzell: It's putting in the specific regions, perhaps?109201:49:34.190 --> 01:49:34.800Nick Coggins: Yep.109301:49:39.220 --> 01:49:43.750Josh Penzell: Activation instructions. So now, let's go ahead and copy this one.109401:49:49.650 --> 01:49:51.610Josh Penzell: Let's start a new chat window.109501:49:58.170 --> 01:50:00.430Josh Penzell: Still, right? Not giving me what I want.109601:50:03.710 --> 01:50:05.379Josh Penzell: It might be because…109701:50:05.760 --> 01:50:10.820Josh Penzell: Yeah. So I don't know why you're not doing it, so I might even say, hey, Why?109801:50:11.640 --> 01:50:22.469Josh Penzell: what do I need to… Tell you, so you just start the convo without all this summarization stuff.109901:50:23.470 --> 01:50:24.680Josh Penzell: You're annoying me.110001:50:35.160 --> 01:50:38.510Josh Penzell: Okay, so let's see if… Alright, let's go back here.110101:50:40.230 --> 01:50:42.729Josh Penzell: It doesn't seem like that would solve it, frankly.110201:50:48.060 --> 01:50:51.550Josh Penzell: One of the things we can always do is go back and edit a message.110301:50:52.400 --> 01:50:57.999Josh Penzell: didn't do what I wanted, so I'm gonna try just removing this voice prompt.110401:50:59.550 --> 01:51:06.050Josh Penzell: And maybe say something like, start… role-playing, Immediately.110501:51:07.920 --> 01:51:10.310Josh Penzell: This is where we're prompt engineering, right?110601:51:15.950 --> 01:51:16.830Josh Penzell: Perfect.110701:51:28.080 --> 01:51:28.820Josh Penzell: Okay?110801:51:29.710 --> 01:51:37.410Josh Penzell: Now, we could keep role-playing with it to make sure it acts the way I want it to, which is probably what we're going to want to do. That's the user experience part of it.110901:51:38.140 --> 01:51:43.399Josh Penzell: But let's say, okay, this is good enough, now I want to roll this out. We want to do this on our audio right now.111001:51:46.520 --> 01:51:52.739Josh Penzell: These things called GPTs, All I want you to think about is a GPT, essentially, is,111101:51:52.930 --> 01:51:56.230Josh Penzell: It's an agent, it's, it's my own…111201:51:56.720 --> 01:52:01.339Josh Penzell: my own chat that I've given it a, an onboarding document, right? I've given it a prompt.111301:52:04.560 --> 01:52:06.809Josh Penzell: So we just created a prompt, right?111401:52:08.090 --> 01:52:10.509Josh Penzell: I'm gonna go in here, let me do this again, just really quickly.111501:52:12.030 --> 01:52:20.359Josh Penzell: Sometimes there's a way to just create one over here, but I'm gonna go to Explore GPTs, I'm gonna create a new GPT. Now, I can go in here and start typing.111601:52:20.460 --> 01:52:23.089Josh Penzell: I don't recommend that, we just created a whole prompt.111701:52:23.340 --> 01:52:24.120Josh Penzell: Right?111801:52:24.330 --> 01:52:25.909Josh Penzell: Let's go ahead and paste.111901:52:26.640 --> 01:52:28.040Josh Penzell: Our buddy in here.112001:52:31.600 --> 01:52:32.789Josh Penzell: What is your name?112101:52:36.140 --> 01:52:37.669Josh Penzell: Let's give a description.112201:52:41.690 --> 01:52:44.650Josh Penzell: Let's have it create an image for our little logo.112301:52:50.960 --> 01:52:54.580Josh Penzell: Either way, while it's working on that, let's just see if… oh, there we go.112401:52:58.780 --> 01:53:01.420Josh Penzell: Here's our program. Still working.112501:53:02.900 --> 01:53:03.650Josh Penzell: Okay?112601:53:06.800 --> 01:53:15.139Josh Penzell: Let's go back to… All right, there's… there's our, that's great.112701:53:15.700 --> 01:53:16.550Josh Penzell: Okay?112801:53:18.150 --> 01:53:21.309Josh Penzell: it doesn't need any specific knowledge, but I could always…112901:53:22.410 --> 01:53:25.499Josh Penzell: give it one. Like, you know, here's…113001:53:25.640 --> 01:53:28.150Josh Penzell: vocab list, here's something I need to give it.113101:53:28.150 --> 01:53:28.740Nick Coggins: Right.113201:53:29.980 --> 01:53:32.320Josh Penzell: And I'm just gonna go ahead and hit Create.113301:53:33.040 --> 01:53:36.669Josh Penzell: And I'm gonna let anyone who has a link access this.113401:53:41.320 --> 01:53:42.529Josh Penzell: Copy this.113501:53:45.300 --> 01:53:46.650Josh Penzell: Here's the TL.113601:53:51.000 --> 01:53:53.210Josh Penzell: And let me go ahead and see if I can pull it up.113701:53:53.730 --> 01:53:55.260Josh Penzell: And see how it talks to me.113801:53:57.580 --> 01:54:00.049Josh Penzell: And we can see, you know.113901:54:01.050 --> 01:54:05.230Josh Penzell: in a few minutes, how we were able to create that. Alright, let's see if you're gonna be able to hear.114001:54:10.340 --> 01:54:13.500Josh Penzell: estamos para divertir nos un ratato.114101:54:18.010 --> 01:54:18.680Josh Penzell: Bye.114201:54:21.370 --> 01:54:22.959Josh Penzell: Pana, por favor.114301:54:24.440 --> 01:54:27.200Josh Penzell: vamos en espano entonces.114401:54:27.310 --> 01:54:28.749Josh Penzell: Vale, pues vamos a darling.114501:54:29.930 --> 01:54:30.680Josh Penzell: I'm a boy.114601:54:34.080 --> 01:54:34.859Josh Penzell: And Madrid?114701:54:36.220 --> 01:54:47.419Josh Penzell: Los siento, pero no hablo muy bien, y muy, muy mas despel tu rapido. Slow down.114801:55:03.810 --> 01:55:20.949Nick Coggins: That's incredible. You know, Dr. Chen in the last class was talking about how educators are just consumers and customers, but they have to be designers, and I think he just illustrated exactly how… why wait for something to do what you're trying to do? Go build it. Go figure it out.114901:55:21.920 --> 01:55:29.079Josh Penzell: Not only that, but see, now… We want to take this, Now I have the instructions.115001:55:30.480 --> 01:55:33.660Josh Penzell: Now I could tape this and figure out, for instance.115101:55:33.800 --> 01:55:36.870Josh Penzell: How do I put you into a program that does audio?115201:55:37.340 --> 01:55:40.580Josh Penzell: If I wanted. But you don't even have to pay for that at this point, right?115301:55:40.620 --> 01:55:41.370Nick Coggins: Okay.115401:55:43.110 --> 01:55:46.090Josh Penzell: But let's say you did, I mean…115501:55:46.310 --> 01:55:48.229Josh Penzell: It's still building stuff for us.115601:55:51.530 --> 01:55:54.330Josh Penzell: Our research is still going on 30 minutes later.115701:55:57.890 --> 01:56:02.530Josh Penzell: So yeah, so we really quickly were able to just create that. Let's go to that website it created for us.115801:56:04.750 --> 01:56:07.800Josh Penzell: Right? Here's the website it created with all of the information.115901:56:08.820 --> 01:56:09.350Nick Coggins: Wow.116001:56:11.910 --> 01:56:14.290Josh Penzell: And by the way, you could say turn this into another language.116101:56:14.560 --> 01:56:16.300Josh Penzell: Immediately, right?116201:56:17.990 --> 01:56:21.010Josh Penzell: So, just because we just did a ton.116301:56:22.960 --> 01:56:26.639Josh Penzell: And we're still going through them. Let me see if I can get back to,116401:56:29.750 --> 01:56:33.910Josh Penzell: Are there any questions, first of all? I mean, I'm sure there's a million, but.116501:56:33.910 --> 01:56:34.670mcred: Well, what?116601:56:34.670 --> 01:56:36.770Josh Penzell: Masterspaccio, por favor, that's the one.116701:56:36.770 --> 01:56:50.690mcred: One… one… and I'm… that's me, my brain is trying to catch up. You've still got these… these agents that are doing research for you, and… and yet you've moved way beyond that, so what's the purpose of having the research continue? What are you going to do with it?116801:56:50.690 --> 01:56:54.730Josh Penzell: Yeah, yeah, yeah. I'm moving on right now because of the time.116901:56:55.020 --> 01:57:00.830Josh Penzell: Just because I'm trying to fit everything in. So, there were a few things I… I would have probably waited.117001:57:01.480 --> 01:57:04.509Josh Penzell: Gotten the research and pasted that all in.117101:57:04.800 --> 01:57:08.280Josh Penzell: But, let's go ahead and show what I would do right now with it.117201:57:08.980 --> 01:57:11.350Josh Penzell: Here's an example of some of that research.117301:57:15.380 --> 01:57:18.310Josh Penzell: So I'll go ahead and download that as a PDF document.117401:57:23.320 --> 01:57:26.209Josh Penzell: And… Here's another one.117501:57:27.490 --> 01:57:33.380Josh Penzell: Let's go ahead and download… That one…117601:57:36.930 --> 01:57:41.369Josh Penzell: And now let's go to, this conversation partner.117701:57:43.340 --> 01:57:44.520Josh Penzell: we were building.117801:57:47.900 --> 01:57:49.099Josh Penzell: I can get to it.117901:57:53.000 --> 01:57:53.740Josh Penzell: Okay.118001:57:56.370 --> 01:57:58.239Josh Penzell: Let's see if I…118101:58:01.530 --> 01:58:04.360Josh Penzell: Sorry, trying to get to the right place.118201:58:05.700 --> 01:58:10.529Josh Penzell: As we wanted. Okay, not that's not right.118301:58:13.160 --> 01:58:16.279Josh Penzell: Let's say we're here, and let's say this was the one we were working on.118401:58:17.300 --> 01:58:25.489Josh Penzell: I apologize, I should be able to find the one I was on, but I can't. Let's… oh, it was this one. Alright, so we're here. Now I would say something like,118501:58:26.430 --> 01:58:31.379Josh Penzell: Using the research, you just discovered…118601:58:32.260 --> 01:58:38.790Josh Penzell: plus the additional research, how could we make this experience.118701:58:39.040 --> 01:58:40.450Josh Penzell: more effective.118801:58:40.710 --> 01:58:43.690Josh Penzell: and make… our tool.118901:58:44.570 --> 01:58:47.499Josh Penzell: Or pedagogically sound.119001:58:55.150 --> 01:58:59.770Josh Penzell: I'll go ahead, and wherever those are… Upload those…119101:59:09.750 --> 01:59:20.830Josh Penzell: And then I would essentially use this to try to improve, right, the quality of whatever it is I was doing. And I would spend more time going through all of these and saying,119201:59:21.870 --> 01:59:24.709Josh Penzell: For instance, let's upload some more stuff here.119301:59:25.610 --> 01:59:41.739mcred: Well, so examples, so example, just to jump in. So, let's say you're pulling in stuff from Vygotsky as a for instance, and I'm the one who's struggling as a less knowledgeable peer in Spanish. Is that information going to help build a tool that119401:59:41.760 --> 01:59:53.220mcred: will then help me build up my conversational ability, because it recognizes that I'm struggling, and it's going to help give me the scaffolding to get me to where I can catch up with it.119501:59:53.550 --> 02:00:04.669Josh Penzell: that's what I'm trying to do, is give the computer program that we're building knowledge about all the different pedagogical approaches, so that it can adapt to us in real time.119602:00:05.600 --> 02:00:10.290Josh Penzell: Right? That being said, it's gonna take… I mean, that's what we're doing, is trying to figure that out.119702:00:10.500 --> 02:00:12.069Josh Penzell: But if I go to my…119802:00:12.650 --> 02:00:15.629Josh Penzell: Computer program that we're building right here.119902:00:17.770 --> 02:00:22.090Josh Penzell: Let me see if I can just find one that's, like, actually… None.120002:00:27.800 --> 02:00:29.509Josh Penzell: Let's go to Replit, because I like that one.120102:00:32.650 --> 02:00:35.430Josh Penzell: What I would say, by the way, is exactly what you just said to it.120202:00:36.780 --> 02:00:39.979Josh Penzell: which I can't repeat as eloquently as you.120302:00:41.230 --> 02:00:49.229Josh Penzell: But I would literally say to it, I'm giving you more… Research and data.120402:00:49.510 --> 02:00:59.279Josh Penzell: And based on this, I want you to be able to adapt your teaching To the learner, student.120502:00:59.430 --> 02:01:01.970Josh Penzell: So that…120602:01:09.620 --> 02:01:13.350Josh Penzell: And then I would probably, let's see, attach… Put a file…120702:01:20.710 --> 02:01:26.899Josh Penzell: And that's what I would do, is add that in to help it. If we wanted, we could go to even the small one we were…120802:01:27.770 --> 02:01:29.659Josh Penzell: Just playing with over here.120902:01:30.240 --> 02:01:31.489Josh Penzell: Let's go ahead and upload that.121002:01:35.450 --> 02:01:37.180Josh Penzell: The one that just gave us a prompt.121102:01:37.860 --> 02:01:47.289Josh Penzell: And we could say to it, hey, how would I do this differently if… and we would say exactly what I just said to you. So essentially, the reason I'm using the… I'm getting more research because I can.121202:01:47.550 --> 02:01:54.650Josh Penzell: and then I would use that however I need to provide more insight to my AI to help.121302:01:55.420 --> 02:01:59.750Josh Penzell: I think the idea, though, is I don't have to wait, I can get that information.121402:02:00.330 --> 02:02:19.699Josh Penzell: Right? And by the way, you can create 5 different programs at once and go back and use it. So I guess what I'm suggesting is more information at the beginning is good. We want to be able to innovate and iterate and experiment, and then what we're doing over time is limiting and having it give us the ideas we need.121502:02:19.810 --> 02:02:24.349Josh Penzell: So, for instance, if that idea came to you while you're doing the research, like, that's…121602:02:24.630 --> 02:02:27.500Josh Penzell: you know, it served its purpose. Sorry, go ahead.121702:02:27.500 --> 02:02:41.179mcred: playing devil's advocate. Please. How do we know that the research results that we're getting is solid, that we're now fading into a development program, and not something that's total BS?121802:02:41.510 --> 02:02:51.780Josh Penzell: Yeah, which is why, right, a few things. First of all, I'm moving quickly here, but I would want to go through and double-check all of the citations, just like I normally would.121902:02:51.920 --> 02:02:56.120Josh Penzell: Right? One thing I would probably do to reduce that122002:02:57.010 --> 02:03:04.899Josh Penzell: is I would go to one of my research briefs, I'd provide the other ones, and then I'd say something to it, like… of course, it's all screwing up here.122102:03:06.790 --> 02:03:13.119Josh Penzell: But I would say something to it, like, look through all these research and find the areas where it's not true, or find the areas where it's biased.122202:03:13.560 --> 02:03:18.130Josh Penzell: I would literally try to use the AI, then to go through its own research.122302:03:18.960 --> 02:03:20.909Josh Penzell: And find the areas where it's wrong.122402:03:20.910 --> 02:03:26.910mcred: So it's kind of like an AI peer review, or an AI Monte Carlo simulation, almost.122502:03:26.910 --> 02:03:33.700Josh Penzell: That's exact… it's exactly what I'll do, and you can also have it simulate that a million times. However, your point is still…122602:03:33.850 --> 02:03:35.399Josh Penzell: It still comes up to the human.122702:03:36.280 --> 02:03:41.260Josh Penzell: Right? Like, the… that… that… the new skill is that 10%, that 5%.122802:03:41.490 --> 02:03:46.569Josh Penzell: of when do I stop, and what information do I buy? Because I can provide you citations to anything.122902:03:46.750 --> 02:03:52.669Josh Penzell: Alright, I mean, I can make a website out there, but critical thinking is, which ones do I actually want to listen to? Which ones…123002:03:52.770 --> 02:03:59.600Josh Penzell: are reasonable, right? Which sources do I believe? All of that right now seems to be up in the air.123102:03:59.880 --> 02:04:13.429Josh Penzell: So, I think that's the skill, essentially, is you can do all this research, so do it, but then it's up to you to figure out which ones do you like the most. I'll give you a very… if I can just get to it, a very tangible example of that.123202:04:13.540 --> 02:04:15.209Josh Penzell: My Chrome just crashed.123302:04:17.580 --> 02:04:18.380Josh Penzell: But…123402:04:25.000 --> 02:04:31.199Josh Penzell: It's not gonna come up. I have a… I'll send it out. I have a… AI,123502:04:31.350 --> 02:04:35.249Josh Penzell: ChatGPT, a GPT, one of those custom ones. Let me see if I can…123602:04:39.580 --> 02:04:41.940Josh Penzell: And what it does is you give it information.123702:04:42.580 --> 02:04:48.940Josh Penzell: It reads that information, and it first identifies Who would be reading this?123802:04:49.930 --> 02:05:05.219Josh Penzell: The reason I have it do that without providing it the information is because I want to see what a third party would perceive as getting this information, right? So a press release, or content. I want to see what an unknown third party would… how they would react to it. And here it's coming back up.123902:05:06.800 --> 02:05:09.600Josh Penzell: So let's just pull up a… I don't know.124002:05:10.550 --> 02:05:13.740Josh Penzell: curriculum standards, for something.124102:05:17.800 --> 02:05:20.170Josh Penzell: And I'm just gonna pull up… let's see…124202:05:31.050 --> 02:05:34.940Josh Penzell: Let's just do a, it was easy on us.124302:05:42.210 --> 02:05:44.530Josh Penzell: Alright, so I'm just gonna get some content.124402:05:44.840 --> 02:05:51.399Josh Penzell: There's a press release, Just to demonstrate this, here is…124502:05:52.770 --> 02:05:55.889Josh Penzell: a tool that I've created, right? And essentially, this is just a big prompt.124602:05:56.370 --> 02:06:01.409Josh Penzell: looks at the content, and it first determines, who do I think is the intended audience?124702:06:02.010 --> 02:06:07.140Josh Penzell: Based on that, It then creates… fake people.124802:06:08.070 --> 02:06:10.000Josh Penzell: Gives them backstories information.124902:06:11.560 --> 02:06:13.549Josh Penzell: It then puts them into a focus group.125002:06:14.520 --> 02:06:17.399Josh Penzell: It provides me a quote-unquote summary of their findings.125102:06:18.580 --> 02:06:21.279Josh Penzell: and then gives me recommended actions. Now.125202:06:22.520 --> 02:06:36.390Josh Penzell: The value here is not that this was all correct, and I assume that even if we did this a thousand times, we couldn't say it's statistically significantly correct that those people would think that, right? That's where a Monte Carlo might fall flat.125302:06:36.760 --> 02:06:38.740Josh Penzell: But, I could say to this.125402:06:41.970 --> 02:06:46.970Josh Penzell: Right? And so what it's really trying to help me do is figure out where the holes are in my knowledge.125502:06:47.930 --> 02:06:49.560Josh Penzell: what I'm not thinking of.125602:06:52.000 --> 02:06:57.370Josh Penzell: The problem is that, essentially, this is a critical… not a problem. The challenge is that's a critical thinking skill.125702:06:58.640 --> 02:07:02.759Josh Penzell: Right? Because I have to know to keep asking it, to keep challenging it, to not accept it.125802:07:09.240 --> 02:07:14.340Josh Penzell: We have 7 minutes, remaining, and I have a feeling that I've, I've,125902:07:15.370 --> 02:07:29.709Josh Penzell: I don't know. There's a lot of silence, maybe that's because people are overwhelmed, maybe that's because they're underwhelmed, maybe it's because I'm blowing minds, I'm not sure. I'm gonna give you the entire webpage so you can dig into this more, and again, if you give me a few days, the transcript will be in there.126002:07:31.210 --> 02:07:34.510mcred: Well, again, I seem to be the one that's the devil's.126102:07:34.510 --> 02:07:35.610Josh Penzell: Go ahead, please.126202:07:35.610 --> 02:07:38.460mcred: My brain is a human brain.126302:07:38.460 --> 02:07:38.890Josh Penzell: Yes.126402:07:38.890 --> 02:07:46.759mcred: Do you have an issue with doing all this stuff that you make your computer crash? Because it just can't keep up.126502:07:49.510 --> 02:07:51.510Josh Penzell: Tell me, tell me, tell me more about that.126602:07:51.510 --> 02:07:56.710mcred: Are there limitations on that, on what kind of computer you have, what kind of bandwidth you have?126702:07:56.940 --> 02:08:00.180Josh Penzell: No, I'm… I mean, I'm running a whole bunch of stuff that wouldn't…126802:08:00.290 --> 02:08:08.119Josh Penzell: affect you, and, you know, the reason is I'm broadcasting right now, I have some stuff running in the background, but we are gonna hit a power, but…126902:08:08.460 --> 02:08:10.250Josh Penzell: You know, we're gonna hit a power limit.127002:08:10.360 --> 02:08:13.850Josh Penzell: Before we hit anything else, but everything I've just done, you can do on your phone.127102:08:14.750 --> 02:08:16.070Josh Penzell: And it's for free.127202:08:17.100 --> 02:08:23.020Josh Penzell: Right, so I think one of the things to… can contend with is that if I can do that, all your students are going to be able to do that.127302:08:23.360 --> 02:08:28.860Josh Penzell: You know? And so how do we deal in that world? And by the way.127402:08:30.210 --> 02:08:33.920Josh Penzell: I don't disagree with you, you are a human, and it acts like a human, too.127502:08:34.760 --> 02:08:38.630Josh Penzell: That's part of the difficulty with it, and so it's not replacing127602:08:39.180 --> 02:08:46.700Josh Penzell: it won't do the same work faster, because it'll just do crappier… the same work, right? It could create an entire lesson plan, but it's not going to be…127702:08:47.590 --> 02:08:50.900Josh Penzell: It's a lesson plan, it's not about actually helping people learn.127802:08:51.260 --> 02:08:56.910Josh Penzell: So, the whole point of AI is not to use it to just replace the current work we're doing.127902:08:57.360 --> 02:09:01.900Josh Penzell: But to use it to challenge our thoughts, to challenge our ideas, to challenge the way we think.128002:09:02.450 --> 02:09:06.860Josh Penzell: We're going to hit a power limit before we hit a processing limit.128102:09:07.780 --> 02:09:17.130Josh Penzell: Right? And the only benefit I can see to that is that capitalism is… Capitalism is,128202:09:17.460 --> 02:09:19.449Josh Penzell: very strong force.128302:09:19.610 --> 02:09:23.430Josh Penzell: Right? And that through that, we'll get renewable resources and renewable energy.128402:09:26.880 --> 02:09:29.019Josh Penzell: But I think the core of your question128502:09:29.160 --> 02:09:37.120Josh Penzell: which is, like, how can I trust it? Am I just abdicating myself to it? I think that is the danger.128602:09:39.200 --> 02:09:53.899Josh Penzell: But with all due respect to generative AI, or maybe all due respect to humanity, humanity's had no problem doing that without generative AI, right? And so I think that's sort of the thing to keep in mind, is it's actually more like a human than a computer.128702:09:54.000 --> 02:09:56.569Josh Penzell: And that means that if I start128802:09:57.670 --> 02:10:03.909Josh Penzell: If I give it more information in one window, where I keep giving it, it's now clouded by the previous information.128902:10:05.800 --> 02:10:10.079Josh Penzell: And so one thought, then, is how much do I try to, you know, bounce among them?129002:10:18.540 --> 02:10:20.989Josh Penzell: If we're gonna create something in our image.129102:10:21.420 --> 02:10:24.880Josh Penzell: We should be prepared for similar consequences.129202:10:25.470 --> 02:10:30.800Josh Penzell: to anyone or anything else that's created something in their image. You get what I mean?129302:10:32.970 --> 02:10:36.010Josh Penzell: Yes, you were gonna say something, follow, and it says Mick Red, so I don'.129402:10:36.010 --> 02:10:37.850mcred: Wow. Just wow.129502:10:37.970 --> 02:10:39.919mcred: That's all I have to say.129602:10:41.160 --> 02:10:47.690Josh Penzell: Good. Again, if it's trained on the core of human knowledge.129702:10:49.870 --> 02:10:52.289Josh Penzell: And it's trained to act like a human.129802:10:53.380 --> 02:10:56.760Josh Penzell: And it speaks… It's human language.129902:11:00.100 --> 02:11:06.059Josh Penzell: Human behavior experts are the people who best know how to use this. It really just requires us unlearning.130002:11:06.320 --> 02:11:08.480Josh Penzell: A lot of habits we've had to develop.130102:11:09.900 --> 02:11:14.920Josh Penzell: That limit the amount of ways we can reach each of our people individually.130202:11:15.140 --> 02:11:15.890Josh Penzell: Right?130302:11:16.470 --> 02:11:18.429Josh Penzell: And allowing them to rethink.130402:11:22.680 --> 02:11:25.570Josh Penzell: I'll just say, yeah, it is overwhelming.130502:11:27.940 --> 02:11:32.809Josh Penzell: my suggestion, find one tool, and just… just, like, one AI,130602:11:32.980 --> 02:11:38.080Josh Penzell: Whichever one it is, Copilot, ChatGPT, And just talk to it.130702:11:38.700 --> 02:11:40.680Josh Penzell: And the next time you have an idea.130802:11:41.450 --> 02:11:44.189Josh Penzell: And your brain says, I wonder if AI could…130902:11:44.510 --> 02:11:49.049Josh Penzell: Or, how might I? Or, it would be great if…131002:11:53.350 --> 02:11:57.509Josh Penzell: Open up your phone, your computer, whatever it is, and either chat.131102:11:58.000 --> 02:12:01.000Josh Penzell: Or talk, or whatever works best for you.131202:12:01.630 --> 02:12:03.149Josh Penzell: And don't worry.131302:12:03.720 --> 02:12:13.539Josh Penzell: About asking it the right question, or wasting money, or wasting time, or any of that. Because as you just saw, in 2 hours, we can iterate a lot.131402:12:13.780 --> 02:12:20.840Josh Penzell: And we've learned a ton, even if none of the stuff we created actually sticks around, because that is the learning.131502:12:21.780 --> 02:12:32.949Josh Penzell: And I would just challenge you to say, how can I now reach every individual out there? Just one thing, I'm neurodiverse, right? I'm autistic. I'm one of the fortunate ones who was able to make, you know, to, like.131602:12:33.520 --> 02:12:37.060Josh Penzell: get employed and all this, but I now think about131702:12:39.020 --> 02:12:43.839Josh Penzell: What, you know, how much different my educational experience might have been,131802:12:43.990 --> 02:12:48.140Josh Penzell: Both for good or bad. Because the dopamine hit on this thing, I'll tell you, is crazy.131902:12:48.830 --> 02:12:56.239Josh Penzell: You know? So, I just think about that, and I think this is an entire change, and as educators, the first thing we have to do132002:12:56.660 --> 02:13:00.059Josh Penzell: Is… Go back to the core, which is, like, we're trying to get132102:13:00.680 --> 02:13:09.589Josh Penzell: We're trying to get people, humans, children, to effectively be able to act on their own, do their own stuff, generalize information, and to succeed in life.132202:13:10.170 --> 02:13:13.229Josh Penzell: Like, not just to know something, but to do something.132302:13:13.370 --> 02:13:24.580Josh Penzell: And so, like, how many of those experiences… and by the way, I bet if you let them do this, they'll create all the cha- they'll solve everything for us. You know what I mean? So we just get out of their way. So…132402:13:25.210 --> 02:13:28.740Josh Penzell: That being said, it still helps to know how to use a slide rule.132502:13:28.910 --> 02:13:34.259Josh Penzell: Right? And, like, you know, a card catalog, so you understand how things work.132602:13:34.320 --> 02:13:48.769Josh Penzell: All right, we're at time. I really appreciate it. Julio will send out the website with all the information. You have my, my information. I'll put in my LinkedIn information here, but again, if this was overwhelming, good.132702:13:48.770 --> 02:13:54.770Josh Penzell: Shake yourselves out of that complacency, and go read, like, go connect with arts and creativity.132802:13:54.860 --> 02:13:58.990Josh Penzell: Like, go reread that book, go reread Siddhartha, go to the art museum.132902:13:59.550 --> 02:14:03.329Josh Penzell: That's the future. That's our goal, so… Thank you so much, everyone.133002:14:03.330 --> 02:14:26.580Julio Zelaya: Thank you very much, my dear Josh. Thank you to everyone. There was a great session, and this week we'll continue our deep dive when one of the world's experts in analytics, and also in computer science, Dr. Ryan Baker, will join us this Wednesday. So, thank you very much, my friends. See you Wednesday. Have a great night, and I hope133102:14:26.610 --> 02:14:33.139Julio Zelaya: You keep on learning and keep on practicing. That's the idea. Have a great night, everybody. Bye-bye.